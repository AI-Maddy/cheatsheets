âœ… **Verification: Proving Your Software Works (Are We Building It Right?)** (2026 Edition!)
==============================================================================================

**Quick ID:** "Are we building it right?" â€” Testing, reviews, static analysis, coverage
**Key Methods:** Unit tests, integration tests, design reviews, code reviews, static analysis, structural coverage
**Coverage Targets:** MC/DC 100% (DAL A/B), Decision 100% (DAL C), Statement (DAL D/E)
**Criticality Level:** â­â­â­â­â­ CRITICALâ€”Verification is how you prove compliance

---

âœˆï¸ **WHAT IS VERIFICATION?**
============================

**Verification** = Confirming software **MEETS ITS REQUIREMENTS** (not whether those requirements are correct)

Simple analogy:
  âœ… Requirement: "Altimeter shall display altitude Â±100 feet accuracy"
  âœ… Verification: Testing proves altimeter displays within Â±100 feet (requirement satisfied)
  âŒ Validation: Proving pilots can actually fly using this altitude (separate activity)

**Core Principle:** Every requirement must have proof it worksâ€”tests, reviews, or analysis.

---

ğŸ“Š **VERIFICATION METHODS: What You Do**
========================================

**1ï¸âƒ£ REVIEWS (Documented proof that requirements are met)**

ğŸ”´ **Software Functional Requirements Review (SFR)**
  Purpose: Verify HLRs are correct, complete, traceable
  Check: Every system requirement â†’ corresponding HLR
  Result: Baseline HLRs, traceability matrix, review minutes

ğŸ”´ **Preliminary Design Review (PDR)**
  Purpose: Verify design satisfies HLRs
  Check: HLRs â†’ Design architecture, modules, interfaces
  Result: Approved design, interface specifications, risk mitigation

ğŸ”´ **Critical Design Review (CDR)**
  Purpose: Final design approval before coding
  Check: Complete design, all requirements covered, safety analysis
  Result: Design baseline, implementation readiness

ğŸ”´ **Code Review**
  Purpose: Verify code meets design and standards (MISRA C/C++)
  Check: Coding standards, complexity limits, traceability to design
  Result: Code review minutes, approval to commit

ğŸ”´ **Final Verification Review (FVR)**
  Purpose: Confirm all verification objectives met
  Check: All tests passed, coverage targets met, traces complete
  Result: Verification complete, ready for validation

---

**2ï¸âƒ£ TESTING (Executable proof)**

ğŸŸ  **Unit Testing**
  Scope: Individual functions/modules
  Method: White-box (access to code internals)
  Coverage: Normal, boundary, error conditions
  Example: Test `check_rpm()` with inputs: -100, 0, 5000, 65535 (boundary values)

ğŸŸ  **Integration Testing**
  Scope: Module-to-module interfaces
  Method: "Do modules work together correctly?"
  Coverage: Parameter passing, timing, state transitions
  Example: Test `Engine_Monitor` â†’ `Alert_System` (pressure reading â†’ alert generation)

ğŸŸ  **System Integration Testing**
  Scope: Full system (all modules integrated)
  Method: "Do all modules work as one cohesive system?"
  Coverage: End-to-end workflows, performance, error handling
  Example: Fly aircraft with test software (full system verification)

---

**3ï¸âƒ£ STRUCTURAL COVERAGE ANALYSIS (Proof that all code paths tested)**

ğŸŸ  **Statement Coverage (Simplest)**
  What: Every line of code executed at least once
  Target: 100% (all code tested)
  Example:
    ```c
    if (rpm > 5000) {
        alert_flag = 1;    // This line must execute
    }
    fail_count++;          // This line must execute
    ```
  Reality: Easy to achieve (87%+ typical), but doesn't test logic branches

ğŸŸ  **Decision Coverage (Intermediate)**
  What: Every decision (if/else) both true & false
  Target: 100% (all branches taken)
  Example:
    ```c
    if (rpm > 5000)        // Test TRUE (rpm=6000)
        alert_flag = 1;    // AND FALSE (rpm=3000)
    ```
  Reality: Catches more issues than statement (harder, ~70% typical)

ğŸŸ  **Modified Condition/Decision Coverage (MC/DC) â€” Most Rigorous**
  What: Every condition affects decision independently
  Target: 100% (required for DAL A/B)
  Example:
    ```c
    if ((rpm > 5000) AND (temp > 200)) {  // 2 conditions
        alert_flag = 1;
    }
    ```
  MC/DC requires: Each condition changes decision outcome (independently)
    âœ… rpm > 5000=TRUE, temp > 200=TRUE â†’ alert
    âœ… rpm > 5000=FALSE, temp > 200=TRUE â†’ no alert (rpm condition caused change)
    âœ… rpm > 5000=TRUE, temp > 200=FALSE â†’ no alert (temp condition caused change)
    âœ… rpm > 5000=FALSE, temp > 200=FALSE â†’ no alert

  Reality: Most rigorous, requires careful test case design (~40% typical by effort)

---

**4ï¸âƒ£ STATIC ANALYSIS (Automated code inspection)**

ğŸŸ¡ **Compiler Warnings**
  What: Compiler flags suspicious code (unused variables, type mismatches)
  Example: `warning: unused variable 'tmp'`
  Action: Fix or document why warning is acceptable

ğŸŸ¡ **MISRA C/C++ Checkers**
  What: Automated tools check for unsafe coding patterns
  Examples:
    âŒ No global variables (reduces side effects)
    âŒ No pointer arithmetic (prevents buffer overflows)
    âŒ All loops have exit conditions (prevents infinite loops)
  Tools: Synopsys CodeSonar, PRQA QA-C, Coverity

ğŸŸ¡ **Static Analyzers (Deep Analysis)**
  What: Advanced tools detect logic errors, buffer overflows, etc.
  Example: "Variable 'ptr' dereferenced after null check (impossible)"
  Tools: Clang Static Analyzer, Coverity, SonarQube

---

**5ï¸âƒ£ FORMAL METHODS (Mathematical proof â€” rare but powerful)**

ğŸŸ¢ **Theorem Proving**
  What: Mathematical proof of algorithm correctness
  Example: "Function `multiply(a,b)` always produces correct result for all integer inputs"
  Tools: Frama-C, TLA+, Coq
  Reality: Expensive, only for critical algorithms (rare in aviation)

---

âš™ï¸ **VERIFICATION PROCESS: The Workflow**
==========================================

**Phase 1: Planning (Month 1â€“2)**
  ğŸ“‹ Create Software Verification Plan (SVP)
     â†’ Test strategy (what will be tested, how)
     â†’ Coverage targets (MC/DC 100% for DAL A/B)
     â†’ Independence requirements (verifier â‰  developer)
     â†’ Tools (VectorCAST, QualityLogic, test framework)
  ğŸ“‹ Create Test Case Specification
     â†’ Template for test cases (requirement ID, inputs, expected output)

**Phase 2: Development (Month 3â€“10)**
  ğŸ“‹ Developers write code
  ğŸ“‹ Developers run unit tests (catch bugs early)
  ğŸ“‹ Integration testing (modules together)
  ğŸ“‹ Structural coverage analysis (ongoing)

**Phase 3: Verification (Month 9â€“18)**
  ğŸ“‹ Complete unit testing
  ğŸ“‹ Complete integration testing
  ğŸ“‹ Analyze structural coverage (100% line coverage, MC/DC analysis)
  ğŸ“‹ Conduct verification reviews (SFR, PDR, CDR, FVR)
  ğŸ“‹ Document objective evidence (test results, coverage reports)

**Phase 4: Validation (Month 16â€“20)**
  ğŸ“‹ System-level testing (high-level requirements validation)
  ğŸ“‹ Aircraft/simulator testing
  ğŸ“‹ Authority observation (SOI #3 gate)

---

ğŸ“Š **COVERAGE TARGETS BY DAL**
==============================

| **Coverage Type** | **DAL A** | **DAL B** | **DAL C** | **DAL D/E** |
|:-----------------|:----------|:----------|:----------|:------------|
| **Statement** | 100% âœ… | 100% âœ… | 100% âœ… | 100% âœ… |
| **Decision** | 100% âœ… | 100% âœ… | 100% âœ… | Optional |
| **MC/DC** | 100% âœ… | 100% âœ… | N/A | N/A |
| **Minimum Path** | 100% âœ… | 100% âœ… | 100% âœ… | Not req'd |
| **Tools** | Required âœ… | Required âœ… | Optional | Optional |
| **Automated** | Yes âœ… | Yes âœ… | Yes âœ… | Developer choice |

---

âš¡ **VERIFICATION VS. VALIDATION: Don't Confuse Them!**
======================================================

| **Aspect** | **Verification** | **Validation** |
|:-----------|:-----------------|:----------------|
| **Question** | "Are we building it right?" | "Are we building the right thing?" |
| **Focus** | Requirements â†’ Code | Intended use â†’ System |
| **Methods** | Tests, reviews, analysis | System testing, aircraft demo |
| **Who** | Developers, testers, SQA | Independent test team, authorities |
| **Proof** | Test results, coverage, reviews | Flight test data, system demos |
| **Timing** | During development | After development, before certification |
| **Example** | Unit test proves function works per spec | Aircraft flight test proves system safe to fly |

**Real Example:**
  âœ… **Verification:** "Autopilot maintains altitude within Â±50 feet (requirement)" â†’ Unit test proves this
  âœ… **Validation:** "Pilots can fly aircraft safely with autopilot engaged" â†’ Flight test proves this

---

ğŸ’¡ **VERIFICATION BEST PRACTICES**
==================================

âœ… **Tip 1: Plan verification upfront (don't defer testing)**
  âŒ Mistake: "We'll figure out testing in Phase 4"
  âœ… Right: SVP created in Month 1, test cases by Month 4
  Impact: Tests ready as code completes, faster verification

âœ… **Tip 2: Test as code is written (continuous verification)**
  âŒ Mistake: "We'll test all 50,000 lines at the end"
  âœ… Right: Unit tests written with each function (test-driven development)
  Impact: Bugs caught early (cheap fix), not late (expensive rework)

âœ… **Tip 3: Separate verifiers from developers (independence)**
  âŒ Mistake: Developer writes code AND tests it
  âœ… Right: Test team writes tests (independent of development)
  Impact: Catches developer blind spots, auditors love this

âœ… **Tip 4: Automate coverage analysis (don't do manually)**
  âŒ Mistake: "We'll measure coverage manually"
  âœ… Right: VectorCAST runs at every build (automated)
  Impact: Coverage gaps discovered immediately, not at end

âœ… **Tip 5: Trace every test to requirement (prove coverage)**
  âŒ Mistake: "We have 1,000 tests; obviously requirements covered"
  âœ… Right: Requirement ID â†’ Test case ID traceability matrix
  Impact: Auditors can verify every requirement tested

âœ… **Tip 6: Document reviews formally (objective evidence)**
  âŒ Mistake: "We did code review, everyone understood"
  âœ… Right: Formal review minutes (attendees, findings, resolutions)
  Impact: Auditor can verify review happened

---

âš ï¸ **COMMON VERIFICATION MISTAKES**
===================================

âŒ **Mistake 1: Testing only "happy path"**
  Problem: Tests only normal conditions, miss boundary/error cases
  Impact: Bugs in edge cases discovered post-deployment
  Fix: Include boundary value tests (min, max, zero, one-less-than-max)

âŒ **Mistake 2: Inadequate coverage targets**
  Problem: "We'll aim for 80% coverage" (insufficient for DAL A/B)
  Impact: Auditor fails verification (requires 100%)
  Fix: Know your DAL targets; plan for MC/DC 100% if DAL A/B

âŒ **Mistake 3: Deferring coverage analysis**
  Problem: "We'll measure coverage after all testing complete"
  Impact: Discovering gaps at audit (expensive rework!)
  Fix: MC/DC analysis continuous (automated at every build)

âŒ **Mistake 4: No independence in verification**
  Problem: Developer verifies their own code (DAL A/B)
  Impact: Common-mode failures, auditor failure
  Fix: Separate verifier (different person/team, trained independently)

âŒ **Mistake 5: Untestable code by design**
  Problem: "This code path is 'unreachable'"
  Impact: Can't achieve 100% coverage (design flaw!)
  Fix: Design for testability upfront (simple, clear logic)

---

ğŸ“ **LEARNING PATH: Mastering Verification**
=============================================

**Week 1: Fundamentals**
  ğŸ“– Read: DO-178C Part 8 (Verification objectives, methods)
  ğŸ“– Study: Coverage types (statement, decision, MC/DC definitions)
  ğŸ¯ Goal: Understand what verification is, why it matters

**Week 2: Planning**
  ğŸ“– Read: DO-178C Part 8 (Verification process, reviews)
  ğŸ“– Study: Test planning (strategy, case development, independence)
  ğŸ¯ Goal: Understand how to plan verification

**Week 3: Practice**
  ğŸ’» Write: Unit tests for sample function (practice coverage thinking)
  ğŸ’» Tool: Set up coverage analyzer (VectorCAST demo, or CodeCov)
  ğŸ¯ Goal: See how coverage analysis works practically

**Week 4: Deep Dive**
  ğŸ“š Study: Real project verification plan (SVP, test cases)
  ğŸ“š Analyze: Coverage report from completed project (see 100% MC/DC)
  ğŸ¯ Goal: Understand how verification works on real projects

---

âœ¨ **BOTTOM LINE**
=================

**Verification = proof that software meets requirements.**

âœ… Multiple methods (tests, reviews, analysis) provide redundant proof
âœ… Coverage targets depend on DAL (100% MC/DC for DAL A/B)
âœ… Independence crucial (verifier â‰  developer, especially higher DALs)
âœ… Continuous (not deferred to end) = faster, cheaper detection of issues
âœ… Objective evidence required (test results, coverage reports, review minutes)

**Remember:** Every requirement must have proof it works! ğŸ¯

---

**Last updated:** 2026-01-12 | **Verification: Proof Your Software Works**

**Key Takeaway:** ğŸ’¡ **Verification = Insurance against hidden bugs.** Invest in testing, coverage analysis, and reviews upfrontâ€”it's the best money you'll spend! âœ…
