â˜ï¸ **Cloud-Native Architecture for Aircraft Systems**
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

**Last Updated:** January 2026  
**Target Role:** Aircraft Services Architect  
**Relevance:** Kubernetes/containers replacing hypervisors in modern IFE systems

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‹ **TABLE OF CONTENTS**
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Overview & Context
2. Why Cloud-Native for Aircraft?
3. Kubernetes Variants (K3s, K8s, OpenShift)
4. Container Orchestration Fundamentals
5. Microservices Architecture for IFE
6. Service Mesh (Istio, Linkerd)
7. Storage & Persistence
8. Security & Isolation
9. CI/CD Pipelines
10. Monitoring & Observability
11. Practical Examples
12. Common Pitfalls
13. Quick Reference Card
14. Exam Questions
15. Further Reading

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ **TL;DR â€” Quick Takeaways**
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âœ… **K3s** (lightweight Kubernetes) for aircraft edge computing (<512 MB RAM vs. 4 GB for K8s)  
âœ… **Containers** (Docker, Podman) replace VMs for faster startup (<5s vs. ~60s)  
âœ… **Microservices** decompose IFE monoliths (video, audio, UI, backend services)  
âœ… **Helm charts** for repeatable deployments across aircraft fleets  
âœ… **Service mesh** (Istio) provides mTLS, traffic shaping, zero-trust networking  
âœ… **Auto-scaling** based on passenger load (30 vs. 300 passengers = 10x resource difference)  
âœ… **Edge-optimized** storage (local PVC with ReadWriteOnce, no cloud NFS)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¢ **1. OVERVIEW & CONTEXT**
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

**Traditional vs. Cloud-Native**

+--------------------------------+--------------------------------+
| Legacy IFE (Pre-2020)          | Cloud-Native IFE (2024+)       |
+================================+================================+
| Monolithic C++ application     | Microservices (Go, Java, C++)  |
| Single-server deployment       | Container orchestration (K3s)  |
| Manual updates (USB stick)     | OTA with Helm/ArgoCD           |
| No horizontal scaling          | Auto-scale based on load       |
| Fixed resource allocation      | Dynamic resource limits (QoS)  |
| Proprietary OS (VxWorks)       | Linux + Kubernetes             |
+--------------------------------+--------------------------------+

**Aircraft-Specific Constraints:**

ğŸ›‘ **No Internet Access** (in-flight):
   - Air-gapped container registry (Harbor)
   - Pre-loaded images on aircraft storage
   
ğŸ”‹ **Power Budget** (typically 500W for IFE):
   - Idle pods must suspend (CPU throttling)
   
ğŸ“¡ **Satellite Bandwidth** (~10 Mbps shared):
   - OTA updates during ground connection only

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â˜ï¸ **2. WHY CLOUD-NATIVE FOR AIRCRAFT?**
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

**Benefits:**

1. **Resource Efficiency**
   - Containers share kernel (no hypervisor overhead)
   - Bin-packing: 20 containers on 1 server vs. 5 VMs
   
2. **Rapid Updates**
   - Blue-green deployments (zero downtime)
   - Rollback in <30 seconds
   
3. **Developer Productivity**
   - Standardized Docker images (dev = prod)
   - CI/CD integration (GitLab, GitHub Actions)
   
4. **Vendor Independence**
   - CNCF standards (not locked to AWS/Azure)
   - Multi-vendor hardware support

**Challenges:**

âŒ **Complexity** (Kubernetes learning curve)  
âŒ **Debugging** (logs across 50+ containers)  
âŒ **Security** (container escape risks)  
âŒ **Certification** (DO-178C not written for containers)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ—ï¸ **3. KUBERNETES VARIANTS**
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

**K3s (Recommended for Aircraft)**

- **Lightweight:** 512 MB RAM, 200 MB binary (vs. 4 GB for K8s)
- **Single-node** or **multi-node** (HA with 3 masters)
- **Edge-optimized:** No cloud dependencies (etcd â†’ SQLite)
- **Built-in Helm controller** (no separate Tiller)

.. code-block:: bash

   # Install K3s (aircraft master node)
   curl -sfL https://get.k3s.io | sh -
   
   # Check status
   kubectl get nodes

**K8s (Standard Kubernetes)**

- Full-featured distribution
- **Heavy:** 4 GB RAM minimum
- Requires external etcd cluster
- Best for ground data centers

**OpenShift (Red Hat)**

- Enterprise-grade (built on K8s)
- Integrated CI/CD (Tekton pipelines)
- **Expensive licensing** (~$100K/year per cluster)

**Comparison Table:**

+----------------+--------+-------+-----------+------------------+
| Feature        | K3s    | K8s   | OpenShift | Aircraft Fit     |
+================+========+=======+===========+==================+
| RAM footprint  | 512 MB | 4 GB  | 8 GB      | âœ… K3s wins      |
| Startup time   | <30s   | ~2min | ~5min     | âœ… K3s wins      |
| Security       | Basic  | RBAC  | RBAC+SEL  | âš ï¸ Add Istio     |
| Certification  | None   | None  | None      | âŒ All need docs |
+----------------+--------+-------+-----------+------------------+

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ›ï¸ **4. CONTAINER ORCHESTRATION FUNDAMENTALS**
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

**Core Concepts:**

ğŸ“¦ **Pod:**
   - Smallest deployable unit (1+ containers)
   - Shared network namespace (localhost communication)
   - Example: IFE UI (nginx) + API sidecar

ğŸ”„ **Deployment:**
   - Manages replica sets (desired state = 3 replicas)
   - Rolling updates (maxUnavailable: 1, maxSurge: 1)

ğŸŒ **Service:**
   - Stable IP/DNS for pod groups
   - Load balancing (round-robin by default)
   - Types: ClusterIP, NodePort, LoadBalancer

ğŸ—‚ï¸ **ConfigMap & Secret:**
   - Configuration decoupled from code
   - Secrets base64-encoded (not encrypted by default!)

**Example Deployment:**

.. code-block:: yaml

   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: ife-video-service
   spec:
     replicas: 3
     selector:
       matchLabels:
         app: video
     template:
       metadata:
         labels:
           app: video
       spec:
         containers:
         - name: video-server
           image: harbor.aircraft.local/ife/video:v2.3
           ports:
           - containerPort: 8080
           resources:
             requests:
               memory: "256Mi"
               cpu: "500m"
             limits:
               memory: "512Mi"
               cpu: "1000m"
           env:
           - name: DB_HOST
             valueFrom:
               configMapKeyRef:
                 name: ife-config
                 key: database_host

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ”€ **5. MICROSERVICES ARCHITECTURE FOR IFE**
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

**Service Decomposition:**

::

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                  API Gateway (Kong)                     â”‚
   â”‚             (Authentication, Rate Limiting)             â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                   â”‚                   â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
   â”‚ Video Serviceâ”‚   â”‚  Audio Service  â”‚  â”‚ UI Service  â”‚
   â”‚   (Go, gRPC) â”‚   â”‚ (C++, REST API) â”‚  â”‚ (React+Nginxâ”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
          â”‚                    â”‚                   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Metadata DB     â”‚
            â”‚  (MySQL Cluster) â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

**Service Responsibilities:**

1. **Video Service** (port 8080):
   - Streams movies (H.265 codec)
   - Adaptive bitrate (4K â†’ 720p on congestion)
   
2. **Audio Service** (port 8081):
   - Music library, podcasts
   - Low-latency audio sync
   
3. **UI Service** (port 80):
   - Web frontend (HTML5, CSS, JS)
   - Touch interface for seat screens
   
4. **Auth Service** (port 8082):
   - Passenger ID validation
   - Session management

**Inter-Service Communication:**

.. code-block:: yaml

   # Service mesh traffic routing (Istio)
   apiVersion: networking.istio.io/v1alpha3
   kind: VirtualService
   metadata:
     name: video-routing
   spec:
     hosts:
     - video-service
     http:
     - match:
       - headers:
           x-passenger-tier:
             exact: premium
       route:
       - destination:
           host: video-service
           subset: 4k-streams
     - route:
       - destination:
           host: video-service
           subset: standard

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ•¸ï¸ **6. SERVICE MESH (ISTIO, LINKERD)**
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

**What is a Service Mesh?**

- **Sidecar proxies** (Envoy) injected into every pod
- Handles: mTLS, retries, circuit breaking, observability
- **Zero code changes** (network layer magic)

**Istio Architecture:**

::

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚              Control Plane                  â”‚
   â”‚  (Istiod: Pilot, Citadel, Galley)           â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ (config push)
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              â”‚              â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
   â”‚ Pod A   â”‚    â”‚ Pod B   â”‚   â”‚ Pod C   â”‚
   â”‚ â”Œâ”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â” â”‚
   â”‚ â”‚ App â”‚ â”‚    â”‚ â”‚ App â”‚ â”‚   â”‚ â”‚ App â”‚ â”‚
   â”‚ â””â”€â”€â”¬â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”¬â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”¬â”€â”€â”˜ â”‚
   â”‚ â”Œâ”€â”€â–¼â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â–¼â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â–¼â”€â”€â” â”‚
   â”‚ â”‚Envoyâ”‚ â”‚â—„â”€â”€â”€â”¼â”€â”¤Envoyâ”‚â”€â”¼â”€â”€â–ºâ”‚ â”‚Envoyâ”‚ â”‚
   â”‚ â””â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”˜ â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

**Benefits:**

âœ… **Mutual TLS** (automatic certificate rotation)  
âœ… **Traffic shifting** (A/B testing, canary releases)  
âœ… **Fault injection** (test resilience)  
âœ… **Distributed tracing** (Jaeger integration)

**Example: Circuit Breaker**

.. code-block:: yaml

   apiVersion: networking.istio.io/v1alpha3
   kind: DestinationRule
   metadata:
     name: video-circuit-breaker
   spec:
     host: video-service
     trafficPolicy:
       connectionPool:
         tcp:
           maxConnections: 100
         http:
           http1MaxPendingRequests: 10
           maxRequestsPerConnection: 2
       outlierDetection:
         consecutiveErrors: 5
         interval: 30s
         baseEjectionTime: 30s
         maxEjectionPercent: 50

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ’¾ **7. STORAGE & PERSISTENCE**
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

**Storage Classes:**

1. **Local PV** (fastest, not portable):
   - SSD/NVMe on aircraft server
   - ReadWriteOnce (single-node access)
   
2. **NFS** (not recommended for aircraft):
   - Requires network, adds latency
   
3. **Rook-Ceph** (distributed storage):
   - Replicates data across nodes (HA)
   - Overhead: ~20% of raw capacity

**Persistent Volume Claim (PVC):**

.. code-block:: yaml

   apiVersion: v1
   kind: PersistentVolumeClaim
   metadata:
     name: ife-media-storage
   spec:
     accessModes:
     - ReadWriteOnce
     storageClassName: local-ssd
     resources:
       requests:
         storage: 2Ti  # 2TB for movie library

**StatefulSets (For Databases):**

.. code-block:: yaml

   apiVersion: apps/v1
   kind: StatefulSet
   metadata:
     name: mysql-cluster
   spec:
     serviceName: mysql
     replicas: 3
     selector:
       matchLabels:
         app: mysql
     template:
       metadata:
         labels:
           app: mysql
       spec:
         containers:
         - name: mysql
           image: mysql:8.0
           ports:
           - containerPort: 3306
           volumeMounts:
           - name: data
             mountPath: /var/lib/mysql
     volumeClaimTemplates:
     - metadata:
         name: data
       spec:
         accessModes: ["ReadWriteOnce"]
         resources:
           requests:
             storage: 100Gi

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ”’ **8. SECURITY & ISOLATION**
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

**Network Policies (Zero Trust):**

.. code-block:: yaml

   apiVersion: networking.k8s.io/v1
   kind: NetworkPolicy
   metadata:
     name: isolate-passenger-services
   spec:
     podSelector:
       matchLabels:
         tier: passenger
     policyTypes:
     - Ingress
     - Egress
     ingress:
     - from:
       - podSelector:
           matchLabels:
             tier: passenger
     egress:
     - to:
       - podSelector:
           matchLabels:
             tier: database
       ports:
       - protocol: TCP
         port: 3306

**Pod Security Standards:**

.. code-block:: yaml

   # Restrict privileged containers
   apiVersion: v1
   kind: Pod
   metadata:
     name: secure-ife-pod
   spec:
     securityContext:
       runAsNonRoot: true
       runAsUser: 1000
       fsGroup: 2000
     containers:
     - name: app
       securityContext:
         allowPrivilegeEscalation: false
         readOnlyRootFilesystem: true
         capabilities:
           drop:
           - ALL

**Image Scanning (Trivy):**

.. code-block:: bash

   # Scan for vulnerabilities
   trivy image harbor.aircraft.local/ife/video:v2.3
   
   # Output: HIGH: 0, MEDIUM: 2, LOW: 15

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ”„ **9. CI/CD PIPELINES**
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

**GitOps with ArgoCD:**

::

   Developer Push    Git Repo      ArgoCD        Kubernetes
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   git push origin   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   main              â”‚ GitLab  â”‚â”€â”€â–ºâ”‚ ArgoCD  â”‚â”€â”€â–ºâ”‚ K3s     â”‚
                     â”‚         â”‚   â”‚ (Sync)  â”‚   â”‚ Cluster â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

**Example GitLab CI/CD Pipeline:**

.. code-block:: yaml

   # .gitlab-ci.yml
   stages:
     - build
     - test
     - deploy
   
   build-image:
     stage: build
     script:
       - docker build -t $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA .
       - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
   
   unit-tests:
     stage: test
     script:
       - go test ./...
   
   deploy-staging:
     stage: deploy
     script:
       - kubectl config use-context staging-cluster
       - helm upgrade --install ife-app ./helm-chart \
           --set image.tag=$CI_COMMIT_SHA
     only:
       - main

**Helm Chart Structure:**

::

   helm-chart/
   â”œâ”€â”€ Chart.yaml
   â”œâ”€â”€ values.yaml
   â”œâ”€â”€ templates/
   â”‚   â”œâ”€â”€ deployment.yaml
   â”‚   â”œâ”€â”€ service.yaml
   â”‚   â””â”€â”€ ingress.yaml

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š **10. MONITORING & OBSERVABILITY**
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

**Stack: Prometheus + Grafana + Loki**

1. **Prometheus** (metrics):
   - Scrapes `/metrics` endpoints
   - CPU, memory, request latency
   
2. **Grafana** (dashboards):
   - Visualizes metrics, alerts
   
3. **Loki** (logs):
   - Aggregates logs from all pods

**Example Prometheus Query:**

.. code-block:: promql

   # Average request latency (last 5 min)
   rate(http_request_duration_seconds_sum[5m]) /
   rate(http_request_duration_seconds_count[5m])

**Distributed Tracing (Jaeger):**

.. code-block:: go

   // Instrument Go service
   import "github.com/opentracing/opentracing-go"
   
   span := opentracing.StartSpan("video-fetch")
   defer span.Finish()
   
   // Trace ID propagates across services
   // View end-to-end latency in Jaeger UI

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ’» **11. PRACTICAL EXAMPLES**
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

**Example 1: Deploy IFE App with Helm**

.. code-block:: bash

   # Add Helm repo (ground network)
   helm repo add ife-charts https://charts.aircraft.local
   
   # Install app
   helm install ife-app ife-charts/video-service \
     --set replicas=5 \
     --set image.tag=v3.1.0 \
     --set resources.limits.memory=1Gi
   
   # Check status
   kubectl get pods -l app=video-service

**Example 2: Auto-Scaling Based on Passenger Load**

.. code-block:: yaml

   apiVersion: autoscaling/v2
   kind: HorizontalPodAutoscaler
   metadata:
     name: ife-autoscaler
   spec:
     scaleTargetRef:
       apiVersion: apps/v1
       kind: Deployment
       name: ife-video-service
     minReplicas: 2
     maxReplicas: 20
     metrics:
     - type: Resource
       resource:
         name: cpu
         target:
           type: Utilization
           averageUtilization: 70

**Example 3: Blue-Green Deployment**

.. code-block:: bash

   # Deploy new version (green)
   kubectl apply -f deployment-green.yaml
   
   # Test green environment
   kubectl port-forward svc/ife-green 8080:80
   curl http://localhost:8080/health
   
   # Switch traffic (update service selector)
   kubectl patch service ife-app -p '{"spec":{"selector":{"version":"green"}}}'
   
   # Rollback if needed
   kubectl patch service ife-app -p '{"spec":{"selector":{"version":"blue"}}}'

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âš ï¸ **12. COMMON PITFALLS**
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âŒ **Pitfall 1: No Resource Limits**
   - **Problem:** One pod consumes all CPU (noisy neighbor)
   - **Solution:** Always set `requests` and `limits`

âŒ **Pitfall 2: Using `latest` Tag**
   - **Problem:** Non-deterministic deployments (which version is running?)
   - **Solution:** Use semantic versioning (e.g., `v2.3.1`)

âŒ **Pitfall 3: Storing Secrets in ConfigMaps**
   - **Problem:** Exposed in etcd (unencrypted by default)
   - **Solution:** Use Secrets + encrypt etcd at rest

âŒ **Pitfall 4: Single-Node Cluster**
   - **Problem:** No high availability (node failure = downtime)
   - **Solution:** 3-node cluster (master + 2 workers)

âŒ **Pitfall 5: Ignoring Pod Disruption Budgets**
   - **Problem:** Rolling update kills all replicas
   - **Solution:** Set `minAvailable: 1`

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‡ **13. QUICK REFERENCE CARD**
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

**Essential kubectl Commands**

.. code-block:: bash

   # Pod management
   kubectl get pods -A                # List all pods
   kubectl describe pod <name>        # Detailed info
   kubectl logs -f <pod> -c <container>  # Stream logs
   kubectl exec -it <pod> -- /bin/sh  # Shell into pod
   
   # Deployments
   kubectl rollout status deployment/ife-app
   kubectl rollout undo deployment/ife-app
   kubectl scale deployment/ife-app --replicas=10
   
   # Services
   kubectl get svc                    # List services
   kubectl port-forward svc/ife-app 8080:80
   
   # Config
   kubectl create configmap app-config --from-file=config.yaml
   kubectl create secret generic db-creds \
     --from-literal=username=admin \
     --from-literal=password=secret
   
   # Debugging
   kubectl top nodes                  # Node resource usage
   kubectl top pods                   # Pod resource usage
   kubectl get events --sort-by=.metadata.creationTimestamp

**Helm Commands**

.. code-block:: bash

   helm list                          # List releases
   helm upgrade <release> <chart>     # Update release
   helm rollback <release> <revision> # Rollback
   helm template <chart>              # Dry-run (preview YAML)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ **14. EXAM QUESTIONS**
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

**Q1:** Why use K3s instead of standard K8s for aircraft?

**A1:** K3s footprint is 512 MB RAM vs. 4 GB for K8s. Aircraft edge computing has limited resources (CPU, power budget). K3s also uses SQLite instead of etcd (no distributed consensus overhead).

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

**Q2:** How does a service mesh (Istio) provide mTLS without code changes?

**A2:** Istio injects an Envoy sidecar proxy into every pod. All traffic is routed through Envoy, which handles TLS handshakes using certificates from Citadel (control plane). Applications communicate over localhost (no TLS in app code).

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

**Q3:** What's the difference between a Deployment and a StatefulSet?

**A3:**  
- **Deployment:** Stateless apps (pods are interchangeable, random names)  
- **StatefulSet:** Stateful apps (stable pod IDs like `mysql-0`, `mysql-1`; persistent volumes follow pods)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

**Q4:** How do you prevent all replicas from being killed during a rolling update?

**A4:** Set a **PodDisruptionBudget** (PDB) with `minAvailable: 1`. This ensures at least 1 pod remains running during voluntary disruptions (updates, drains).

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

**Q5:** Your pod shows `CrashLoopBackOff`. How do you debug?

**A5:**  
1. Check logs: `kubectl logs <pod> --previous` (previous container logs)  
2. Describe pod: `kubectl describe pod <pod>` (events section)  
3. Check liveness probe (might be too aggressive)  
4. Exec into running container: `kubectl exec -it <pod> -- /bin/sh`

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“š **15. FURTHER READING**
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

**Books:**
- *Kubernetes: Up and Running* (3rd Edition) â€” Kelsey Hightower
- *Production Kubernetes* â€” Josh Rosso, Rich Lander
- *Istio in Action* â€” Christian Posta, Rinor Maloku

**Standards:**
- CNCF Cloud Native Glossary: https://glossary.cncf.io
- Kubernetes Best Practices: https://kubernetes.io/docs/concepts/

**Online:**
- K3s docs: https://docs.k3s.io
- Helm charts: https://artifacthub.io
- Istio patterns: https://istio.io/latest/docs/

**Courses:**
- CKAD (Certified Kubernetes Application Developer)
- CKA (Certified Kubernetes Administrator)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… **COMPLETION CHECKLIST**
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

- [ ] Deploy K3s on aircraft hardware
- [ ] Create Helm chart for IFE application
- [ ] Configure HorizontalPodAutoscaler
- [ ] Set up Istio service mesh with mTLS
- [ ] Implement blue-green deployment
- [ ] Configure persistent storage (local PV)
- [ ] Set up Prometheus/Grafana monitoring

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

**Document Status:** âœ… COMPLETE  
**Created:** January 14, 2026  
**For:** Aircraft Services Architect Role (Portland, PAC)  
**Next:** Review [ARINC_664_Cheatsheet.rst](ARINC_664_Cheatsheet.rst)
