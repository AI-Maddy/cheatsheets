**Linux DMA cache handling** + **inter-processor (SMP/multi-core) memory coherence & pitfalls** (kernel perspective, modern kernels 6.1â€“6.14 era, 2025â€“2026 reality).

ðŸŽ“ 1. DMA Cache Coherency â€“ Core Concepts

| Concept                          | Meaning / When it matters                                      | Coherent? | Typical Latency / BW Impact | Linux API / Flag                     |
|----------------------------------|----------------------------------------------------------------|-----------|------------------------------|--------------------------------------|
| **Hardware coherent DMA**        | CPU caches & device see identical view without driver flushing | Yes       | Higher setup cost, lower BW  | ``dma_set_mask_and_coherent(..., 64)`` |
| **Non-coherent / streaming DMA** | Driver must explicitly sync CPU â†” device view                  | No        | Lower latency, higher BW     | ``dma_map_single()``, ``dma_sync_single_*()`` |
| **DMA from device â†’ CPU**        | Device wrote â†’ CPU must see data                               | â€”         | â€”                            | ``dma_sync_single_for_cpu()``          |
| **DMA from CPU â†’ device**        | CPU wrote â†’ device must see data                               | â€”         | â€”                            | ``dma_sync_single_for_device()``       |
| **Bidirectional streaming**      | Both directions â€” rare & expensive                             | â€”         | ðŸ”´ ðŸ”´ Avoid if possible            | ``DMA_BIDIRECTIONAL``                  |

**Golden rule 2026**:  
**Never use coherent mappings for large/high-bandwidth data**  
â†’ Use **streaming** + explicit sync â†’ 2â€“10Ã— better performance on modern ARM/x86 SoCs.

â­ ðŸ“š 2. Most Important DMA Cache APIs

| Function / Call                                      | Purpose                                              | Direction / When to call                     | Common Mistake |
|------------------------------------------------------|------------------------------------------------------|----------------------------------------------|----------------|
| ``dma_alloc_coherent()``                               | Allocate **coherent** buffer + mapping               | Control descriptors, small rings             | Using it for 100+ MiB buffers |
| ``dma_free_coherent()``                                | Free above                                           | â€”                                            | Forgetting to pair |
| ``dma_map_single()``                                   | Map **streaming** single buffer                      | CPU â†’ device or device â†’ CPU                 | Not checking ``DMA_MAPPING_ERROR`` |
| ``dma_unmap_single()``                                 | Unmap above                                          | After transfer complete                      | Wrong direction |
| ``dma_sync_single_for_cpu()``                          | Invalidate CPU cache after device write              | Device â†’ CPU (RX)                            | Missing â†’ stale data |
| ``dma_sync_single_for_device()``                       | Flush CPU cache before device read                   | CPU â†’ device (TX)                            | Missing â†’ device sees garbage |
| ``dma_map_sg()`` / ``dma_unmap_sg()``                    | Scatter-gather version (most high-perf drivers)      | Networking, NVMe, GPU, camera                | Not using ``sg_dma_len()`` |
| ``dma_set_mask()`` / ``dma_set_mask_and_coherent()``     | Tell DMA engine max address bits                     | Probe time â€” prefer 64-bit                   | Falling back to 32-bit |
| ``dma_sync_sg_for_cpu()`` / ``â€¦_for_device()``           | Sync entire scatterlist                              | SG-heavy drivers                             | Partial sync only |

â­ ðŸ›¡ï¸ 3. DMA Direction Enum (Critical for Correctness)

| Direction              | Typical Use                              | Sync Needed? | Cache Action on Sync |
|------------------------|------------------------------------------|--------------|----------------------|
| ``DMA_TO_DEVICE``        | CPU â†’ device (TX, outbound)              | Yes          | Flush / write-back   |
| ``DMA_FROM_DEVICE``      | Device â†’ CPU (RX, inbound)               | Yes          | Invalidate           |
| ``DMA_BIDIRECTIONAL``    | Both directions (rare)                   | Yes          | Flush + invalidate   |
| ``DMA_NONE``             | Debug / placeholder                      | â€”            | â€”                    |

ðŸ’¾ 4. Inter-Processor (SMP) Memory Coherency & Pitfalls

| Concept / Problem              | Description / Symptom                                      | Fix / ðŸŸ¢ ðŸŸ¢ Best Practice (2026)                          |
|--------------------------------|------------------------------------------------------------|-----------------------------------------------------|
| **False sharing**              | Two CPUs write different variables in same cache line      | ``__cacheline_aligned`` / separate cache lines        |
| **Store buffer / write combining** | CPU write not immediately visible to other cores       | ``smp_wmb()`` / ``smp_store_release()``                 |
| **Missing memory barrier**     | Reordering â†’ stale data / use-after-free                   | ``smp_mb()``, ``smp_rmb()``, ``smp_wmb()``                |
| **this_cpu_* operations**      | Lockless per-CPU counters                                  | ``this_cpu_inc()``, ``this_cpu_add()`` â€“ no barrier needed |
| **per-CPU data races**         | Accessing per-CPU variable from wrong CPU                  | ``migrate_disable()`` / ``get_cpu()`` â†’ ``put_cpu()``     |
| **Cache invalidation storm**   | Too many IPIs for TLB/cache flush                          | Batch flushes, use ``on_each_cpu()`` sparingly        |
| **Coherent DMA on SMP**        | Device write â†’ all CPU caches invalidated                  | Very expensive on large systems â†’ prefer streaming  |
| **Atomic operations**          | ``cmpxchg``, ``xchg``, ``atomic_*``                              | Implicit full barriers on most architectures        |

â­ **Essential memory barriers cheat-sheet**

| Barrier              | Ensures                              | Use when                                      |
|----------------------|--------------------------------------|-----------------------------------------------|
| ``smp_mb()``           | Full memory barrier                  | Need total order                              |
| ``smp_wmb()``          | Write-write / write-read order       | Before releasing pointer                      |
| ``smp_rmb()``          | Read-read / read-write order         | After acquiring pointer                       |
| ``smp_store_release()``| Store + release barrier              | Publish pointer safely                        |
| ``smp_load_acquire()`` | Load + acquire barrier               | Consume pointer safely                        |
| ``dma_wmb()`` / ``dma_rmb()`` | DMA-specific (ARM mostly)      | DMA sync points (usually ``dma_sync_*`` better) |

âš¡ 5. Quick Reference â€“ High-Performance DMA Driver Pattern

.. code-block:: c

/* Probe / init */
if (pci_set_dma_mask(pdev, DMA_BIT_MASK(64)) ||
    pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(64)))
    /* fallback to 32-bit or fail gracefully */

pci_set_master(pdev);

/* RX path â€“ streaming, device â†’ CPU */
dma_addr = dma_map_single(&pdev->dev, skb->data, len, DMA_FROM_DEVICE);
if (dma_mapping_error(&pdev->dev, dma_addr)) { â€¦ }

/* After device signals done (interrupt / poll) */
dma_sync_single_for_cpu(&pdev->dev, dma_addr, len, DMA_FROM_DEVICE);
dma_unmap_single(&pdev->dev, dma_addr, len, DMA_FROM_DEVICE);

/* TX path â€“ CPU â†’ device */
dma_sync_single_for_device(&pdev->dev, dma_addr, len, DMA_TO_DEVICE);
dma_unmap_single(â€¦);   // after TX complete

ðŸ› 6. Debugging Commands (2026-era)

.. code-block:: bash

================================================================================
ðŸ› DMA debug (very noisy!)
================================================================================

.. contents:: ðŸ“‘ Quick Navigation
   :depth: 2
   :local:



echo 1 > /sys/module/dma_debug/parameters/enable
dmesg | grep dma-debug

================================================================================
âš™ï¸ Cache behavior / false sharing
================================================================================

perf stat -e cache-misses,cache-references ./myapp
perf record -e mem_load_uops_retired.l3_miss,name=l3miss ./myapp

================================================================================
âš™ï¸ IOMMU / DMA mapping stats
================================================================================

cat /proc/iommu/sva_mappings   # or debugfs/iommu/

================================================================================
ðŸ› Per-CPU variables inspection (crash / ftrace)
================================================================================

crash> per_cpu counters cpu0

**Bottom line 2026**:  
- **Streaming DMA** + **explicit sync** is the performance king  
- **Coherent** only for small control structures  
- **False sharing** and **missing barriers** remain top SMP bugs  
- Use ``this_cpu_*`` and ``smp_*`` barriers religiously in lockless code

ðŸŸ¢ ðŸŸ¢ Good luck with your high-performance DMA / SMP kernel code!

================================================================================

**Last updated:** January 2026

================================================================================

**Last updated:** January 2026

================================================================================

**Last updated:** January 2026
