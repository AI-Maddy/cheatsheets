**cheatsheet** for **Linux kernel memory management internals** focusing on:

- **Buddy page allocator** (lowest-level physical page management)
- **Slab/SLUB allocator** (object caching on top of buddy)
- **mmap / vm_area_struct** (user virtual memory mappings)

Current as of early 2026 (Linux ~6.12‚Äì6.16 era): **SLUB** remains the **default** slab allocator (SLAB deprecated since ~6.5, removed in later versions; SLOB long gone for tiny systems). No fundamental redesign of buddy or mmap, but ongoing refinements (e.g., MTE, large folios, sheaves proposals still experimental/discussion stage in 2025 LSFMM).

üìå 1. Buddy Page Allocator (mm/page_alloc.c)

Manages physical memory in power-of-2 sized blocks (pages usually 4 KiB).

‚≠ê | Concept                  | Description / Key Details                                                                 |
|--------------------------|--------------------------------------------------------------------------------------------|
| **Unit**                 | struct page (one per physical page)                                                        |
| **Buddy system**         | Free pages grouped by order 0‚ÄìMAX_ORDER-1 (~11 on x86_64 ‚Üí 4 MiB max single block)         |
| **Free lists**           | ``free_area[order]`` array per zone; each has ``free_list`` + ``nr_free``                        |
| **Zones**                | NODE_ZONES ‚Üí DMA / DMA32 / NORMAL / MOVABLE / HIGHMEM (32-bit)                             |
| **Main allocation**      | ``__alloc_pages()`` ‚Üí ``get_page_from_freelist()`` ‚Üí try preferred zone ‚Üí fallback zones      |
‚≠ê | **Key GFP flags**        | ``__GFP_DMA``, ``__GFP_HIGHMEM``, ``__GFP_MOVABLE``, ``__GFP_ZERO``, ``__GFP_NOWAIT``, ``__GFP_RETRY_MAYFAIL`` |
| **Watermarks**           | min / low / high per zone; used for throttling / direct reclaim                        |
| **Compaction**           | Moves movable pages to create higher-order contiguous blocks                              |
‚≠ê | **Important functions**  | ``alloc_pages(gfp, order)``<br>``__free_pages(page, order)``<br>``__get_free_pages()``           |
| **Per-CPU caches**       | ``pcp`` (per-cpu-pages): batch of pages per order to reduce zone lock contention             |

**Quick lookup table: order ‚Üí size**

| Order | Pages | Bytes (4 KiB page) | Typical use case                  |
|-------|-------|---------------------|-----------------------------------|
| 0     | 1     | 4 KiB               | Most slab objects, single pages   |
| 3     | 8     | 32 KiB              | Larger slab pages                 |
| 9     | 512   | 2 MiB               | Huge pages (THP) fallback         |
| 11    | 2048  | 8 MiB               | Very large kernel allocations     |

üìå 2. SLUB Allocator (mm/slub.c) ‚Äî Default since ~2.6.23, still in 2026

Replaced SLAB (deprecated) and SLOB (removed). Focus: simplicity, debuggability, low overhead.

‚≠ê | Concept                  | Description / Key Details                                                                 |
|--------------------------|--------------------------------------------------------------------------------------------|
| **Cache**                | ``struct kmem_cache`` ‚Äî one per object type/size (created via ``kmem_cache_create()``)        |
| **Slab page**            | Physical page(s) from buddy (usually order 0‚Äì3); contains many objects                    |
| **Freelist**             | Single linked list per slab page (stored in object metadata or page->freelist)            |
| **Per-CPU**              | Each CPU has ``struct kmem_cache_cpu``: ``page`` (current slab), ``freelist``, ``tid``            |
| **Node lists**           | Partial / full / empty slabs per NUMA node (``kmem_cache_node``)                            |
| **Main APIs**            | ``kmalloc(size, gfp)`` ‚Üí size-indexed cache<br>``kmem_cache_alloc(cache, gfp)``               |
| **Object layout**        | Metadata (freelist pointer + redzone + last bytes tag) embedded or in page struct         |
| **Debug features**       | ``CONFIG_SLUB_DEBUG`` ‚Üí redzoning, poisoning, tracking, freelist randomization              |
| **Fastpath**             | CPU freelist non-empty ‚Üí lockless alloc/free                                              |
| **Slowpath**             | Get new slab from partial list or allocate from buddy ‚Üí ``new_slab()``                      |
‚≠ê | **Key structures**       | ``kmem_cache`` ‚Üí ``kmem_cache_cpu`` ‚Üí ``page`` ‚Üí objects + freelist                             |

**kmalloc size buckets** (common on x86_64, SLUB)

| Request size range       | Cache name     | Object size (approx) |
|--------------------------|----------------|----------------------|
| 1‚Äì8 B                    | kmalloc-8      | 8 B                  |
| 9‚Äì16 B                   | kmalloc-16     | 16 B                 |
| ‚Ä¶                        | ‚Ä¶              | ‚Ä¶                    |
| 257‚Äì512 B                | kmalloc-512    | 512 B                |
| 513 B ‚Äì 1 KiB            | kmalloc-1k     | 1024 B               |
| Larger                   | kmalloc-*      | up to ~8 KiB         |

**kmalloc large objects** (> ~PAGE_SIZE/2): directly ‚Üí buddy via ``kmalloc_large()``

üíæ 3. mmap / Virtual Memory Area (VMA) Internals (mm/mmap.c)

User process virtual address space management.

‚≠ê | Concept                  | Description / Key Details                                                                 |
|--------------------------|--------------------------------------------------------------------------------------------|
| **VMA**                  | ``struct vm_area_struct`` ‚Äî contiguous virtual range with same attributes                   |
| **mm_struct**            | Per-process: owns ``mmap`` (red-black tree of VMAs), page tables, hiwater_rss, etc.         |
| **Main tree**            | ``mm->mm_rb`` (RB-tree ordered by vm_start) + ``mm->mmap`` (linked list)                      |
| **mmap syscall flow**    | ``do_mmap()`` ‚Üí ``vm_area_struct`` alloc ‚Üí ``find_vma()`` check overlap ‚Üí link into tree        |
| **Flags (vm_flags)**     | ``VM_READ``, ``VM_WRITE``, ``VM_EXEC``, ``VM_MAY*``, ``VM_SHARED``, ``VM_GROWSDOWN``, ``VM_HUGEPAGE``   |
| **File-backed mmap**     | ``vm_file`` ‚Üí ``struct file*``, uses ``vm_ops->fault()`` (usually ``filemap_fault()``)            |
| **Anonymous mmap**       | ``vm_file = NULL``, uses demand-zero pages or anonymous COW                                 |
| **Page fault handling**  | ``__do_fault()`` ‚Üí vma->vm_ops->fault() ‚Üí alloc page ‚Üí ``handle_mm_fault()``                  |
‚≠ê | **Key functions**        | ``do_mmap()`` / ``do_munmap()``<br>``find_vma()`` / ``find_vma_intersection()``<br>``vm_brk()``     |
| **Large / THP**          | Transparent Huge Pages: ``khugepaged`` collapses 4 KiB ‚Üí 2 MiB anonymously                  |
| **VMA merging**          | Adjacent VMAs with identical attributes/flags/file are merged automatically               |

**Typical VMA layout (x86_64 user space)**

0x0000000000000000          ‚Üê null page / guard
... mmap_base (randomized)  ‚Üê libraries, mmap()
... heap (grows up via brk)
... stack (grows down)
0x00007fffffffffff          ‚Üê stack top
0xffff...                   ‚Üê kernel (direct map, vmalloc, modules, fixmap)

‚öôÔ∏è 4. Quick Comparison: Allocation Paths

| Allocator       | Physically contiguous? | Virtually contiguous? | Typical size     | Use case examples                     | Backend source          |
|-----------------|------------------------|------------------------|------------------|---------------------------------------|--------------------------|
| Buddy           | Yes                    | No (physical)          | 4 KiB ‚Äì  several MiB | Large DMA, huge pages, slab pages    | Physical RAM            |
| SLUB / kmalloc  | Yes (small)            | Yes                    | ‚â§ ~8 KiB         | Kernel objects, small buffers        | Buddy pages             |
| vmalloc         | No                     | Yes                    | Any (large)      | Modules, large non-contig buffers    | Buddy pages + PTEs      |
| mmap (anon)     | No                     | Yes                    | Any              | Process heap, large anon mappings    | Demand-zero / swap      |
| mmap (file)     | No                     | Yes                    | Any              | Shared libraries, file mappings      | Page cache              |

üêõ 5. Debugging & Tracing Tips (2026-era)

.. code-block:: bash

================================================================================
Slab / SLUB stats
================================================================================

.. contents:: üìë Quick Navigation
   :depth: 2
   :local:



cat /proc/slabinfo
slabtop

================================================================================
Buddy stats per zone
================================================================================

cat /proc/zoneinfo

================================================================================
Active VMAs of a process
================================================================================

cat /proc/<pid>/maps
cat /proc/<pid>/smaps

================================================================================
üêõ Trace faults & allocations
================================================================================

trace-cmd record -e kmem:* -e mm:* -e anon_fault -e filemap_fault
perf record -e kmem:*u -e page-faults

üü¢ üü¢ Good luck kernel hacking ‚Äî start reading ``mm/page_alloc.c``, ``mm/slub.c``, and ``mm/mmap.c`` for deepest understanding!

================================================================================

**Last updated:** January 2026

================================================================================

**Last updated:** January 2026

================================================================================

**Last updated:** January 2026
