**cheatsheet** for **DMA (Direct Memory Access) in the Linux kernel** (as of early 2026, kernels ~6.12‚Äì6.16), focusing on the modern **DMA API** (``include/linux/dma-mapping.h``) together with patterns for writing **low-latency, high-bandwidth device drivers** (e.g. networking, storage, accelerators, FPGA PCIe cards, high-speed ADC/DAC, GPU-related, NVMe).

üéì 1. DMA API Core Concepts

| Concept                  | Description / When to use                                                                 | Coherent? | Typical Latency / Bandwidth Impact |
|--------------------------|-------------------------------------------------------------------------------------------|-----------|-------------------------------------|
| **Coherent DMA**         | CPU & device see the same data without extra flushing/invalidation. Expensive, often page-aligned | Yes       | Higher latency, lower BW overhead   |
| **Streaming DMA**        | One-directional transfers; requires explicit sync (map ‚Üí use ‚Üí sync ‚Üí unmap)             | No        | Lower latency & higher BW possible  |
| **dma_addr_t**           | Bus/device-visible address (may differ from physical/CPU virt addr on IOMMU platforms)    | ‚Äî         | ‚Äî                                   |
| **IOMMU / DMA remapping**| Translates device addresses ‚Üí physical RAM; enables >4 GB DMA on 32-bit capable devices   | ‚Äî         | Adds ~10‚Äì50 ns latency (modern HW)  |
‚≠ê | **Scatter-Gather (SG)**  | Handles non-contiguous buffers via ``struct scatterlist``                                   | ‚Äî         | Essential for high BW (>10 Gbit/s)  |

‚≠ê üìö 2. Key DMA Allocation & Mapping Functions

| Function / Call                              | Purpose                                                                 | Coherent? | GFP flags allowed? | Notes / üü¢ üü¢ Best practice for low-latency/high-BW |
|----------------------------------------------|-------------------------------------------------------------------------|-----------|--------------------|-----------------------------------------------|
| ``dma_alloc_coherent(dev, size, &dma_handle, gfp)`` | Alloc coherent buffer + map                                            | Yes       | GFP_KERNEL / ATOMIC| Use for control descriptors, rings. üî¥ üî¥ Avoid for large data (> few pages) |
| ``dma_free_coherent(dev, size, cpu_addr, dma_handle)`` | Free above                                                     | Yes       | ‚Äî                  | Always pair with alloc                        |
| ``dma_set_mask_and_coherent(dev, mask)``       | Set both streaming & coherent DMA mask (64-bit preferred)               | ‚Äî         | ‚Äî                  | Call in probe(); fail gracefully if !64-bit   |
| ``dma_map_single(dev, ptr, size, DMA_TO_DEVICE)`` | Map single kernel buffer for device write (‚Üí device)               | No        | ‚Äî                  | Fastest single-buffer path; check return != DMA_MAPPING_ERROR |
| ``dma_unmap_single(dev, dma_addr, size, dir)`` | Unmap above                                                            | No        | ‚Äî                  | Required before CPU reuse                     |
| ``dma_sync_single_for_cpu/dev(...)``           | Flush/invalidate cache for streaming mapping                            | No        | ‚Äî                  | Minimize calls; group operations              |
| ``dma_map_sg(dev, sglist, nents, dir)``        | Map scatterlist (most common for high BW)                               | No        | ‚Äî                  | Preferred for networking/storage drivers      |
| ``dma_unmap_sg(...)``                          | Unmap scatter-gather list                                               | No        | ‚Äî                  | ‚Äî                                             |
| ``dma_pool_create(...)``                       | Create pool of small coherent objects (descriptors)                    | Yes       | ‚Äî                  | Better than many dma_alloc_coherent() calls   |
| ``dma_pool_alloc(pool, gfp, &dma_handle)``     | Allocate from pool                                                      | Yes       | Yes                | Low overhead for frequent small allocations   |

‚≠ê **DMA direction** enum values (critical for correctness & optimization):

- ``DMA_BIDIRECTIONAL``
- ``DMA_TO_DEVICE``     (CPU ‚Üí device)
- ``DMA_FROM_DEVICE``   (device ‚Üí CPU)
- ``DMA_NONE``          (debug only)

üèóÔ∏è 3. Low-Latency + High-Bandwidth Driver Patterns (2025‚Äì2026 era)

| Goal / Scenario                              | Recommended Pattern / Technique                                                                 | Why it helps latency & bandwidth                                   | Example drivers (kernel tree) |
|----------------------------------------------|--------------------------------------------------------------------------------------------------|--------------------------------------------------------------------|-------------------------------|
| **Descriptor rings / queues**                | Use ``dma_pool`` for descriptors + coherent or streaming for data payloads                         | Descriptors coherent ‚Üí no sync; data streaming ‚Üí optimal cache use | mlx5, ixgbe, nvme, amdgpu     |
| **Zero-copy / high BW networking**           | ``build_skb()`` + ``dma_map_sg()`` on user pages (or ``page_pool``)                                   | üî¥ üî¥ Avoid memcpy; multi-page SG lists for jumbo frames/GRO             | mlx5, ice, i40e, virtio-net   |
| **Ultra-low latency (FPGA / NIC <5 ¬µs)**     | Pre-allocate & pre-map buffers; use polling instead of interrupts; ``dmaengine`` prep + submit     | üî¥ üî¥ Avoid interrupt + scheduler latency; fixed buffers reduce setup    | fpga-mgr, AXI DMA, some DPDK-like kernel drivers |
| **High bandwidth (>100 Gbit/s or GPU/accel)**| Large SG lists + ``dma_map_sg()``; use huge pages if device supports; üî¥ üî¥ avoid coherent for bulk data | Maximizes PCIe TLP efficiency; reduces IOMMU TLB pressure          | nvme, mlx5, amdgpu, hisi_sas  |
| **Cache-line alignment**                     | Align buffers & descriptors to 64/128 bytes; üî¥ üî¥ avoid false sharing                                | Prevents cache ping-pong between CPU cores & device DMA            | Most high-perf drivers        |
| **Interrupt coalescing + napi**              | Moderate coalescing + busy-poll / NAPI gro polling                                              | Reduces interrupt rate while keeping latency acceptable            | mlx5, ice                     |
‚≠ê | **Polling mode / busy-wait**                 | Use ``dmaengine`` tx/rx submit + poll completion status in tight loop                             | Sub-¬µs tail latency for critical paths (trading CPU %)             | Custom FPGA drivers, some audio/ADC |
| **üî¥ üî¥ Avoid coherent for large data**            | Use streaming + explicit ``dma_sync_*`` only when needed                                          | Coherent often forces write-back / uncached ‚Üí big BW penalty       | Almost all high-BW drivers    |
| **IOMMU bypass / passthrough**               | Enable ``iommu=pt`` kernel cmdline or driver-specific no-iommu mode                               | Removes IOMMU translation latency (~20‚Äì100 ns per transaction)     | Some FPGA & storage drivers   |

üìö 4. Quick Reference: DMAEngine (for offload engines)

Used when hardware has dedicated DMA controller (not PCIe master DMA).

.. code-block:: c

struct dma_chan *chan = dma_request_chan(dev, "rx");     // or "tx"
struct dma_async_tx_descriptor *tx;
dma_cookie_t cookie;

tx = dmaengine_prep_slave_sg(chan, sglist, nents, DMA_DEV_TO_MEM, flags);
cookie = dmaengine_submit(tx);
dma_async_issue_pending(chan);

// Later: dmaengine_terminate_all(chan); or wait via dmaengine_tx_status()

üìå 5. Common Mistakes to üî¥ üî¥ Avoid (2026 perspective)

- Forgetting ``dma_set_mask_and_coherent()`` ‚Üí 32-bit fallback ‚Üí fails on >4 GB systems
- Using coherent mappings for large/high-bandwidth data ‚Üí severe performance loss
- Not checking ``dma_mapping_error()`` after map calls
- Unmapping with wrong direction or size
- Sharing cache lines between CPU-written control + device DMA data
- Excessive ``dma_sync_single_*`` calls in fast path

üêõ 6. Debugging & Tools

.. code-block:: bash

================================================================================
üêõ DMA debug (very noisy!)
================================================================================

.. contents:: üìë Quick Navigation
   :depth: 2
   :local:



echo 8192 > /sys/module/dmaengine/parameters/debug
dmesg -w | grep dma

================================================================================
‚öôÔ∏è IOMMU / mapping stats
================================================================================

cat /proc/iommu/sva_mappings   # or /sys/kernel/debug/iommu/

================================================================================
perf for DMA stalls / PCIe
================================================================================

perf record -e pci:* -e dma:* -- sleep 10
perf report

**üü¢ üü¢ Best starting points** in kernel source (6.x era):

- ``Documentation/core-api/dma-api-howto.rst`` (still the canonical guide)
- ``include/linux/dma-mapping.h``
- ``drivers/net/ethernet/intel/ice/``, ``drivers/net/ethernet/mellanox/mlx5/``, ``drivers/nvme/host/``
- ``drivers/dma/dmaengine.h`` for offload engines

üü¢ üü¢ Good luck writing high-performance DMA drivers!

================================================================================

**Last updated:** January 2026

================================================================================

**Last updated:** January 2026

================================================================================

**Last updated:** January 2026
