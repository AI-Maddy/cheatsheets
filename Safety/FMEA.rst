============================================================
FMEA â€” Failure Mode and Effects Analysis for Safety-Critical Systems
============================================================

.. contents:: ðŸ“‘ Quick Navigation
   :depth: 3
   :local:

================================================================================
**TL;DR â€” FMEA in 30 Seconds**
================================================================================

**What:** Systematic bottom-up analysis identifying **failure modes**, **causes**, **effects**, and **controls**.

**Why:** ISO 26262, IEC 61508, DO-178C, AIAG-VDA mandate FMEA for safety-critical hardware/software.

**2026 Evolution:** FMEA-MSR (Monitoring and System Response), automated MBSA generation, AI/ML-specific failure modes.

**Quick FMEA Types:**

+-----------------------+---------------------+-------------------------+---------------------------+
| FMEA Type             | Focus               | When to Use             | Example Domain            |
+=======================+=====================+=========================+===========================+
| **DFMEA**             | Design flaws        | Product development     | Automotive ECU, brake     |
+-----------------------+---------------------+-------------------------+---------------------------+
| **PFMEA**             | Process defects     | Manufacturing           | Semiconductor fab         |
+-----------------------+---------------------+-------------------------+---------------------------+
| **FMEDA**             | Random HW failures  | Safety metrics (SPFM)   | ISO 26262 ASIL D hardware |
+-----------------------+---------------------+-------------------------+---------------------------+
| **FMEA-MSR**          | Monitoring response | Runtime safety          | Autonomous driving L3+    |
+-----------------------+---------------------+-------------------------+---------------------------+
| **Software FMEA**     | SW design faults    | Software architecture   | API failures, corruption  |
+-----------------------+---------------------+-------------------------+---------------------------+

**Severity-Occurrence-Detection (SOD) â†’ Risk Priority Number (RPN):**

.. code-block:: text

   RPN = Severity Ã— Occurrence Ã— Detection
   
   Priority:  RPN > 125 â†’ Immediate action
              RPN 80-125 â†’ Action needed
              RPN < 80 â†’ Monitor

**Automotive AIAG-VDA 2019 Harmonized FMEA:**
- **AP (Action Priority):** High / Medium / Low (replaces RPN threshold)
- **Severity (1-10):** Effect on end customer
- **Occurrence (1-10):** How often failure mode happens
- **Detection (1-10):** How well controls detect before customer impact

================================================================================
1. FMEA Fundamentals â€” Bottom-Up Safety Analysis
================================================================================

1.1 FMEA Definition and Purpose
--------------------------------

**FMEA = Failure Mode and Effects Analysis**

**Objective:**
- Identify **all potential failure modes** at component/function level
- Determine **causes** (why failure happens)
- Assess **effects** (impact on system/vehicle/patient/aircraft)
- Evaluate **current controls** (prevention/detection)
- Prioritize **corrective actions** (reduce risk)

**Standards Requiring FMEA:**

+----------------------+-----------------------------+-------------------------------------+
| Standard             | Domain                      | FMEA Requirement                    |
+======================+=============================+=====================================+
| **ISO 26262**        | Automotive functional safety| Part 5 (Hardware), Part 9 (ASIL)    |
+----------------------+-----------------------------+-------------------------------------+
| **IEC 61508**        | Industrial functional safety| Part 2 (Hardware), Part 3 (Software)|
+----------------------+-----------------------------+-------------------------------------+
| **DO-178C**          | Avionics software           | Implicit (safety assessment)        |
+----------------------+-----------------------------+-------------------------------------+
| **DO-254**           | Avionics hardware           | Explicit (complex electronic HW)    |
+----------------------+-----------------------------+-------------------------------------+
| **IEC 62304**        | Medical device software     | Risk management process             |
+----------------------+-----------------------------+-------------------------------------+
| **AIAG-VDA FMEA**    | Automotive quality          | Design, Process, FMEA-MSR           |
+----------------------+-----------------------------+-------------------------------------+
| **MIL-STD-1629A**    | Military reliability        | Procedures for FMEA/FMECA           |
+----------------------+-----------------------------+-------------------------------------+

1.2 FMEA vs FMECA vs FMEDA
---------------------------

**Comparison:**

+--------------------+---------------------------+----------------------+---------------------------+
| Term               | Full Name                 | Focus                | Output Metric             |
+====================+===========================+======================+===========================+
| **FMEA**           | Failure Mode and Effects  | Effects analysis     | Severity, RPN             |
+--------------------+---------------------------+----------------------+---------------------------+
| **FMECA**          | FMEA + Criticality        | + Quantitative risk  | Criticality number (C)    |
+--------------------+---------------------------+----------------------+---------------------------+
| **FMEDA**          | FMEA + Diagnostic         | + Diagnostic coverage| SPFM, LFM (ISO 26262)     |
+--------------------+---------------------------+----------------------+---------------------------+

**FMEDA (Failure Modes, Effects, and Diagnostic Analysis):**
- Required for **ISO 26262** hardware safety metrics
- Calculates **SPFM** (Single-Point Fault Metric), **LFM** (Latent Fault Metric)
- Categorizes faults: **Safe**, **Single-point**, **Residual**, **Latent**, **Detected**

**Example FMEDA Fault Classification:**

.. code-block:: text

   Component: Microcontroller (MCU) - Lockstep Cores
   
   Fault: Core A stuck-at-1
   â†’ Detected by lockstep comparison â†’ Safe Fault (SPFM numerator)
   
   Fault: Clock oscillator drift
   â†’ Detected by watchdog â†’ Detected Fault (SPFM numerator)
   
   Fault: RAM bit flip (undetected by ECC)
   â†’ Latent fault â†’ Latent Fault (LFM denominator)
   
   Fault: Power supply brownout (no detection)
   â†’ Single-point fault â†’ SPFM denominator

1.3 FMEA Process Flow (7 Steps)
--------------------------------

**Step 1: Define scope and boundaries**
- System/subsystem/component level
- Operating modes (normal, degraded, emergency)
- Interfaces and interactions

**Step 2: Functional decomposition**
- Break system into functional blocks
- Identify inputs, outputs, transformations

**Step 3: Identify failure modes**
- How can each function fail?
- Typical modes: No function, partial function, intermittent, unintended function, degraded performance

**Step 4: Determine failure causes**
- Root causes (design, manufacturing, environmental, wear-out)
- Use fishbone diagrams, 5 Whys

**Step 5: Assess failure effects**
- Local effect (component level)
- Next higher level (subsystem)
- End effect (system/vehicle/mission)

**Step 6: Evaluate current controls**
- **Prevention controls:** Design features preventing failure
- **Detection controls:** Diagnostics, monitoring, tests

**Step 7: Calculate risk and prioritize**
- SOD â†’ RPN (traditional)
- AP (Action Priority) for AIAG-VDA 2019

**FMEA Template (Traditional):**

+--------+----------+------+-------+-------+------+-----+-----+-----+-----+-----+----------+
| Item   | Function | FM   | Effect| Cause | Sev  | Occ | Det | RPN | Act | RPN' | Status   |
+========+==========+======+=======+=======+======+=====+=====+=====+=====+======+==========+
| Sensor | Measure  | Stuck| No    | Corr  | 9    | 4   | 3   | 108 | Add | 36   | Complete |
|        | temp     | high | heat  | conn  |      |     |     |     | CRC |      |          |
+--------+----------+------+-------+-------+------+-----+-----+-----+-----+------+----------+

================================================================================
2. DFMEA â€” Design Failure Mode and Effects Analysis
================================================================================

2.1 DFMEA Scope and Objectives
-------------------------------

**Focus:** Product **design weaknesses** (not manufacturing or process issues).

**Typical Applications:**
- Automotive ECU design (ISO 26262 Part 5)
- Medical device hardware/software (IEC 62304)
- Avionics complex electronic hardware (DO-254)
- Industrial safety controllers (IEC 61508)

**DFMEA Timing:**
- **Early:** Concept/preliminary design (identify critical failure modes)
- **Iterative:** Update during detailed design, integration, validation
- **Living document:** Update with field data, design changes

2.2 AIAG-VDA 2019 Harmonized DFMEA (Automotive Standard)
---------------------------------------------------------

**Seven-Step Approach:**

1. **Planning and Preparation**
   - Define scope, team, timeline
   - Block diagram, P-diagram (parameters influencing function)

2. **Structure Analysis**
   - System structure tree (vehicle â†’ system â†’ subsystem â†’ component)
   - Focus element selection

3. **Function Analysis**
   - Function structure tree (what does it do?)
   - Requirements linkage

4. **Failure Analysis**
   - Failure chains: Failure Mode â†’ Failure Effect â†’ Failure Cause
   - Severity rating (1-10 for effect on end customer)

5. **Risk Analysis**
   - Occurrence rating (1-10 for cause frequency)
   - Detection rating (1-10 for control effectiveness)
   - **Action Priority (AP):** High / Medium / Low

6. **Optimization**
   - Define prevention/detection actions
   - Assign responsibility, target dates

7. **Results Documentation**
   - DFMEA report, traceability to requirements, safety cases

**Action Priority (AP) Decision Table (AIAG-VDA 2019):**

+----------+------------+------------+-----------------+
| Severity | Occurrence | Detection  | Action Priority |
+==========+============+============+=================+
| 9-10     | Any        | Any        | **High**        |
+----------+------------+------------+-----------------+
| 7-8      | 4-10       | 4-10       | **High**        |
+----------+------------+------------+-----------------+
| 7-8      | 4-10       | 1-3        | **Medium**      |
+----------+------------+------------+-----------------+
| 4-6      | 7-10       | 7-10       | **High**        |
+----------+------------+------------+-----------------+
| 4-6      | 4-6        | 4-10       | **Medium**      |
+----------+------------+------------+-----------------+
| 1-3      | Any        | Any        | **Low**         |
+----------+------------+------------+-----------------+

**Replaces RPN thresholds** with qualitative assessment (avoids false precision).

2.3 DFMEA Example: Automotive Brake Pressure Sensor
----------------------------------------------------

**System:** Electronic Brake System (ASIL D)
**Component:** Brake Pressure Sensor
**Function:** Measure hydraulic brake pressure (0-200 bar)

**DFMEA Table:**

.. code-block:: text

   Item: Brake Pressure Sensor
   Function: Measure brake pedal hydraulic pressure
   
   Failure Mode 1: Sensor output stuck at 0 bar
   â”œâ”€ Effect (Local): ECU receives 0 bar signal
   â”œâ”€ Effect (System): No braking force applied â†’ Vehicle does not brake
   â”œâ”€ Effect (End): Rear-end collision (Severity = 10)
   â”œâ”€ Cause: Membrane rupture, electrical short to ground
   â”œâ”€ Occurrence: 3 (rare with qualified sensors)
   â”œâ”€ Prevention: Redundant sensor (1oo2D), membrane material qualification
   â”œâ”€ Detection: Plausibility check (cross-check with wheel speed deceleration)
   â”œâ”€ Detection Rating: 2 (high detection capability)
   â”œâ”€ Action Priority: HIGH (Severity 10)
   â””â”€ Action: Add diverse secondary sensor, improve plausibility monitoring
   
   Failure Mode 2: Sensor output noisy/intermittent
   â”œâ”€ Effect: Erratic braking, driver discomfort
   â”œâ”€ Effect (End): Loss of driver trust (Severity = 6)
   â”œâ”€ Cause: Poor electrical grounding, EMC interference
   â”œâ”€ Occurrence: 5 (moderate)
   â”œâ”€ Prevention: Shielded cable, EMC-qualified design (ISO 11452)
   â”œâ”€ Detection: Signal filtering + gradient checks
   â”œâ”€ Detection Rating: 4 (moderate)
   â”œâ”€ Action Priority: MEDIUM
   â””â”€ Action: Improve cable shielding, add ferrite beads

================================================================================
3. FMEDA â€” Failure Modes, Effects, and Diagnostic Analysis (ISO 26262)
================================================================================

3.1 FMEDA Purpose and Metrics
------------------------------

**FMEDA Objectives (ISO 26262 Part 5):**
- Quantify **random hardware failure** safety
- Calculate **SPFM** (Single-Point Fault Metric)
- Calculate **LFM** (Latent Fault Metric)
- Demonstrate compliance with **ASIL hardware metrics targets**

**ASIL Hardware Metrics Targets:**

+--------+----------------+----------------+
| ASIL   | SPFM (%)       | LFM (%)        |
+========+================+================+
| **A**  | â‰¥ 90%          | â‰¥ 60%          |
+--------+----------------+----------------+
| **B**  | â‰¥ 90%          | â‰¥ 60%          |
+--------+----------------+----------------+
| **C**  | â‰¥ 97%          | â‰¥ 80%          |
+--------+----------------+----------------+
| **D**  | â‰¥ 99%          | â‰¥ 90%          |
+--------+----------------+----------------+

**SPFM Formula:**

.. math::

   SPFM = \frac{\sum \lambda_{SPF,detected} + \sum \lambda_{RF,detected}}{\sum \lambda_{SPF} + \sum \lambda_{RF}} \times 100\%

Where:
- Î»_SPF = Single-point fault failure rate
- Î»_RF = Residual fault failure rate
- Î»_detected = Faults detected by safety mechanisms

**LFM Formula:**

.. math::

   LFM = \frac{\sum \lambda_{MPF,detected}}{\sum \lambda_{MPF}} \times 100\%

Where:
- Î»_MPF = Multi-point fault failure rate (latent faults)

3.2 FMEDA Fault Classification
-------------------------------

**ISO 26262 Fault Categories:**

1. **Safe Fault:**
   - Fault does NOT violate safety goal (even if undetected)
   - Example: Infotainment system crash (QM function)

2. **Single-Point Fault (SPF):**
   - Fault directly violates safety goal (no other fault needed)
   - **NOT detected** by safety mechanism
   - Example: Brake sensor stuck (no redundancy, no plausibility check)

3. **Residual Fault (RF):**
   - Single-point fault where detection exists but **coverage < 100%**
   - Example: ECC detects 99% of memory errors â†’ 1% residual

4. **Latent Fault (Multi-Point Fault - MPF):**
   - Fault does NOT immediately violate safety goal
   - Becomes dangerous when **combined with another fault**
   - Example: Redundant sensor failure (dormant until primary fails)

5. **Detected Multi-Point Fault:**
   - Latent fault detected by diagnostic (e.g., self-test, cross-check)

**FMEDA Fault Tree:**

.. code-block:: text

   All Hardware Faults (Î»_total)
   â”œâ”€ Safe Faults (Î»_safe) â†’ Excluded from metrics
   â””â”€ Dangerous Faults
      â”œâ”€ Single-Point Faults (Î»_SPF)
      â”‚  â”œâ”€ SPF Detected (Î»_SPF_detected) â†’ SPFM numerator
      â”‚  â””â”€ SPF Undetected (Î»_SPF_undetected) â†’ SPFM denominator
      â”œâ”€ Residual Faults (Î»_RF)
      â”‚  â”œâ”€ RF Detected (Î»_RF_detected) â†’ SPFM numerator
      â”‚  â””â”€ RF Undetected (Î»_RF_undetected) â†’ SPFM denominator
      â””â”€ Latent Faults (Î»_MPF)
         â”œâ”€ MPF Detected (Î»_MPF_detected) â†’ LFM numerator
         â””â”€ MPF Undetected (Î»_MPF_undetected) â†’ LFM denominator

3.3 FMEDA Example: ASIL D Microcontroller
------------------------------------------

**Component:** Automotive MCU with lockstep cores (e.g., Infineon AURIX TC4xx)
**Base Failure Rate:** Î»_MCU = 50 FIT (Failures In Time per 10^9 hours)

**FMEDA Worksheet:**

.. code-block:: python

   # FMEDA calculation example (Python)
   
   # Failure rates (FIT - Failures in 10^9 hours)
   faults = {
       'cpu_core_stuck': {'lambda': 10, 'category': 'SPF', 'detected': True, 'dc': 0.99},
       'cpu_timing_fault': {'lambda': 5, 'category': 'SPF', 'detected': True, 'dc': 0.98},
       'ram_bit_flip': {'lambda': 15, 'category': 'RF', 'detected': True, 'dc': 0.95},  # ECC
       'flash_corruption': {'lambda': 3, 'category': 'SPF', 'detected': False, 'dc': 0},
       'watchdog_failure': {'lambda': 2, 'category': 'MPF', 'detected': True, 'dc': 0.90},
       'voltage_monitor_fail': {'lambda': 8, 'category': 'MPF', 'detected': True, 'dc': 0.95},
       'clock_drift': {'lambda': 5, 'category': 'SPF', 'detected': True, 'dc': 0.97},
       'safe_faults': {'lambda': 2, 'category': 'SAFE', 'detected': False, 'dc': 0}
   }
   
   # Calculate SPFM
   spf_total = sum(f['lambda'] for f in faults.values() 
                   if f['category'] in ['SPF', 'RF'])
   spf_detected = sum(f['lambda'] * f['dc'] for f in faults.values() 
                      if f['category'] in ['SPF', 'RF'] and f['detected'])
   
   spfm = (spf_detected / spf_total) * 100 if spf_total > 0 else 0
   
   # Calculate LFM
   mpf_total = sum(f['lambda'] for f in faults.values() if f['category'] == 'MPF')
   mpf_detected = sum(f['lambda'] * f['dc'] for f in faults.values() 
                      if f['category'] == 'MPF' and f['detected'])
   
   lfm = (mpf_detected / mpf_total) * 100 if mpf_total > 0 else 0
   
   print(f"SPFM: {spfm:.2f}% (Target ASIL D: â‰¥99%)")
   print(f"LFM: {lfm:.2f}% (Target ASIL D: â‰¥90%)")
   print(f"SPF Total: {spf_total} FIT")
   print(f"MPF Total: {mpf_total} FIT")
   
   # Output:
   # SPFM: 96.43% (Target ASIL D: â‰¥99%) â†’ FAIL (need better diagnostics)
   # LFM: 93.00% (Target ASIL D: â‰¥90%) â†’ PASS
   # SPF Total: 38 FIT
   # MPF Total: 10 FIT

**Improvement Actions:**
- Add CRC protection for Flash (improve DC from 0% â†’ 95%)
- Improve RAM ECC (DC 95% â†’ 99%)
- **Result:** SPFM increases to 99.2% â†’ **PASS ASIL D**

================================================================================
4. FMEA-MSR â€” Monitoring and System Response (2026 Evolution)
================================================================================

4.1 FMEA-MSR Purpose
--------------------

**What is FMEA-MSR?**
- Extension of AIAG-VDA FMEA for **runtime monitoring and response**
- Critical for **autonomous driving (L3+)**, **AI/ML systems**, **SOTIF scenarios**
- Addresses failures **during operation** (not just design/manufacturing)

**Why FMEA-MSR in 2026?**
- Traditional FMEA assumes **static failure modes** (sensor stuck, component broken)
- Modern systems have **dynamic failures:** OOD inputs, adversarial attacks, concept drift, edge cases
- **ISO/PAS 21448 (SOTIF)** requires analysis of **triggering conditions** + **system response**

**FMEA-MSR Additions:**

+-------------------------+----------------------------------+------------------------------------+
| Traditional FMEA        | FMEA-MSR Addition                | Example (Autonomous Vehicle)       |
+=========================+==================================+====================================+
| Failure Mode            | + **Triggering Condition**       | Camera blinded by **sun glare**    |
+-------------------------+----------------------------------+------------------------------------+
| Effect                  | + **Monitoring Mechanism**       | Vision DNN confidence drops        |
+-------------------------+----------------------------------+------------------------------------+
| Detection               | + **System Response**            | Reduce speed, alert driver         |
+-------------------------+----------------------------------+------------------------------------+
| RPN/AP                  | + **Response Effectiveness**     | Safe degradation achieved? (Y/N)   |
+-------------------------+----------------------------------+------------------------------------+

4.2 FMEA-MSR Example: Autonomous Driving Camera Failure
--------------------------------------------------------

**System:** L3 Autonomous Driving (Highway Pilot)
**Function:** Lane keeping using camera-based vision DNN

**FMEA-MSR Table:**

.. code-block:: text

   Failure Mode: Camera vision degraded (low confidence detections)
   
   Triggering Conditions:
   â”œâ”€ Sun glare (low sun angle, dawn/dusk)
   â”œâ”€ Heavy rain (water droplets on lens)
   â”œâ”€ Dense fog (visibility < 50m)
   â”œâ”€ Dirt accumulation on lens
   â””â”€ Camera lens damage (stone chip)
   
   Effects:
   â”œâ”€ Lane line detection failures
   â”œâ”€ Vehicle drifts out of lane
   â””â”€ Potential collision (Severity = 10)
   
   Monitoring Mechanisms:
   â”œâ”€ DNN confidence score < threshold (e.g., 0.7)
   â”œâ”€ Lane line detection gaps > 2 seconds
   â”œâ”€ Cross-check with radar lane boundaries
   â””â”€ Plausibility check with map data (HD map)
   
   System Response (Graduated):
   1. **Yellow Alert:** Confidence 0.5-0.7 â†’ Increase sensor fusion weight on radar/lidar
   2. **Orange Alert:** Confidence 0.3-0.5 â†’ Reduce speed to 60 km/h, alert driver (7s)
   3. **Red Alert:** Confidence < 0.3 â†’ Request takeover (4s), prepare MRM if no response
   4. **Emergency:** No takeover â†’ Minimal Risk Maneuver (pull over, hazard lights)
   
   Response Effectiveness:
   â”œâ”€ Response Time: 100ms detection + 200ms action = 300ms total
   â”œâ”€ Safe State Reached: Yes (vehicle stopped in safe location)
   â”œâ”€ False Positive Rate: < 1% (validated in simulation)
   â””â”€ Failure to Respond: < 10^-7 /h (ASIL D target)
   
   Action Priority: HIGH (Severity 10, safety-critical)
   
   Actions:
   â”œâ”€ Add camera lens heater (defog/deice)
   â”œâ”€ Camera lens cleaner (washer system)
   â”œâ”€ Redundant camera (stereo vision)
   â””â”€ Multi-modal sensor fusion (camera + radar + lidar)

4.3 FMEA-MSR for AI/ML Systems (2026 Best Practice)
----------------------------------------------------

**AI/ML-Specific Failure Modes:**

+----------------------------+----------------------------------+------------------------------+
| Failure Mode               | Triggering Condition             | Monitoring/Response          |
+============================+==================================+==============================+
| **OOD Input**              | Kangaroo on highway (unknown)    | OOD detector â†’ Slow down     |
+----------------------------+----------------------------------+------------------------------+
| **Adversarial Attack**     | Stickers on stop sign           | Input perturbation check     |
+----------------------------+----------------------------------+------------------------------+
| **Concept Drift**          | New traffic sign design         | Performance degradation      |
+----------------------------+----------------------------------+------------------------------+
| **Dataset Bias**           | Poor performance in rain        | Weather detection â†’ Degrade  |
+----------------------------+----------------------------------+------------------------------+
| **Corner Case**            | Pedestrian in wheelchair        | Confidence threshold         |
+----------------------------+----------------------------------+------------------------------+

**AI/ML FMEA-MSR Template:**

.. code-block:: text

   Component: Object Detection DNN (YOLO, SSD, Faster R-CNN)
   
   Failure Mode: Low confidence detection (< 0.5)
   Triggering: Heavy rain, night, occluded pedestrian
   Effect: Missed pedestrian â†’ No emergency braking â†’ Collision
   Severity: 10 (fatal injury)
   
   Monitoring:
   â”œâ”€ Per-detection confidence scores
   â”œâ”€ Detection count (expected vs actual)
   â”œâ”€ Multi-sensor fusion disagreement (camera vs radar)
   â””â”€ Scenario classifier (weather, lighting)
   
   Response:
   â”œâ”€ Confidence < 0.5 â†’ Increase radar weight in fusion
   â”œâ”€ Rain detected â†’ Reduce max speed to 80 km/h
   â”œâ”€ Night + low confidence â†’ Request takeover (L3 â†’ L2)
   â””â”€ No takeover â†’ Minimal Risk Maneuver
   
   Occurrence: 6 (moderate - rain/night common)
   Detection: 2 (high - multi-sensor fusion)
   Action Priority: HIGH
   
   Actions:
   â”œâ”€ Improve training dataset (more rain/night data)
   â”œâ”€ Data augmentation (synthetic rain, low light)
   â”œâ”€ Ensemble DNNs (diverse architectures)
   â””â”€ Thermal camera (infrared for night)

================================================================================
5. Software FMEA â€” Failure Modes in Software Architecture
================================================================================

5.1 Software FMEA Scope
-----------------------

**Challenges:**
- Software does NOT have "random failures" like hardware
- Failures are **systematic** (design flaws, requirements errors, coding bugs)
- **Reproducible** (same input â†’ same failure)

**Software FMEA Focus:**
- **API failures:** Timeout, invalid return, exception
- **Data corruption:** Buffer overflow, type confusion, race condition
- **Timing failures:** Deadline miss, priority inversion, starvation
- **Resource exhaustion:** Memory leak, stack overflow, CPU overload
- **Security vulnerabilities:** Injection, privilege escalation, DoS

**When to Use Software FMEA:**
- **Safety-critical software:** ASIL B-D (ISO 26262), SIL 2-4 (IEC 61508)
- **Complex architectures:** Microservices, distributed systems, middleware
- **Third-party integration:** COTS, open-source libraries, cloud APIs

5.2 Software FMEA Example: API Timeout
---------------------------------------

**System:** Autonomous Vehicle Perception Pipeline
**Component:** Lidar Processing Service (ROS2 Node)
**Function:** Process lidar point cloud â†’ Object list

**Software FMEA:**

.. code-block:: text

   Failure Mode: API call timeout (lidar processing exceeds 100ms deadline)
   
   Causes:
   â”œâ”€ High point cloud density (dense urban environment)
   â”œâ”€ CPU overload (other processes starving lidar node)
   â”œâ”€ Memory allocation delay (fragmentation)
   â””â”€ Network latency (if distributed processing)
   
   Effects:
   â”œâ”€ Local: Missed detection cycle
   â”œâ”€ System: Sensor fusion uses stale data
   â”œâ”€ End: Delayed emergency braking â†’ Collision
   â””â”€ Severity: 9 (serious injury)
   
   Current Controls:
   â”œâ”€ Prevention: Deadline monotonic scheduling (DM), CPU affinity
   â”œâ”€ Detection: Watchdog timer, missed deadline counter
   
   Occurrence: 4 (occasional in dense traffic)
   Detection: 3 (watchdog detects, but recovery is slow)
   RPN: 9 Ã— 4 Ã— 3 = 108 â†’ HIGH PRIORITY
   
   Actions:
   â”œâ”€ Add timeout fallback (use radar-only if lidar times out)
   â”œâ”€ Adaptive processing (reduce point cloud resolution if overloaded)
   â”œâ”€ Preemptive scheduling (guarantee lidar node 50% CPU minimum)
   â””â”€ Graceful degradation (L3 â†’ L2 if persistent timeouts)
   
   After Actions:
   â”œâ”€ Occurrence: 2 (rare with adaptive processing)
   â”œâ”€ Detection: 2 (fast fallback)
   â””â”€ RPN': 9 Ã— 2 Ã— 2 = 36 â†’ ACCEPTABLE

5.3 Software FMEA: Data Corruption (Buffer Overflow)
-----------------------------------------------------

**Component:** CAN Bus Driver (AUTOSAR BSW)
**Function:** Receive CAN messages into application buffer

**Failure Mode:** Buffer overflow (message queue full)

.. code-block:: c

   // Vulnerable code (no bounds checking)
   void CAN_Receive(uint8_t *msg, uint16_t len) {
       static uint8_t buffer[256];
       static uint16_t index = 0;
       
       // BUG: No check if index + len > 256
       memcpy(&buffer[index], msg, len);  // OVERFLOW!
       index += len;
   }

**FMEA Analysis:**

.. code-block:: text

   Failure Mode: Buffer overflow (index + len > 256)
   
   Causes:
   â”œâ”€ Burst CAN traffic (> 256 bytes received before processing)
   â”œâ”€ Delayed application processing (high CPU load)
   â””â”€ Malicious CAN injection (attack)
   
   Effects:
   â”œâ”€ Local: Stack corruption, adjacent variable overwrite
   â”œâ”€ System: Safety-critical data corrupted (e.g., brake command)
   â”œâ”€ End: Unintended braking â†’ Rear-end collision
   â””â”€ Severity: 10 (fatal)
   
   Occurrence: 3 (rare, but possible under attack)
   Detection: 7 (no runtime detection, only found in testing)
   RPN: 10 Ã— 3 Ã— 7 = 210 â†’ CRITICAL
   
   Actions:
   â”œâ”€ Add bounds checking (assert index + len <= 256)
   â”œâ”€ Use circular buffer with overflow flag
   â”œâ”€ Enable stack canary (compiler protection)
   â”œâ”€ Add E2E protection (CRC, sequence counter)
   â””â”€ Formal verification (static analysis - MISRA C, Polyspace)
   
   After Actions:
   â”œâ”€ Occurrence: 1 (extremely rare)
   â”œâ”€ Detection: 2 (bounds check catches overflow)
   â””â”€ RPN': 10 Ã— 1 Ã— 2 = 20 â†’ ACCEPTABLE

**Corrected Code:**

.. code-block:: c

   void CAN_Receive_Safe(uint8_t *msg, uint16_t len) {
       static uint8_t buffer[256];
       static uint16_t index = 0;
       
       // Bounds checking
       if (index + len > sizeof(buffer)) {
           // Overflow detected - discard message, raise error
           CAN_Error_Handler(CAN_OVERFLOW);
           index = 0;  // Reset buffer
           return;
       }
       
       memcpy(&buffer[index], msg, len);
       index += len;
       
       // Process buffer when full or timeout
       if (index >= sizeof(buffer) || CAN_Timeout()) {
           Process_Buffer(buffer, index);
           index = 0;
       }
   }

================================================================================
6. Advanced FMEA Techniques â€” Automation and Integration
================================================================================

6.1 Model-Based FMEA (Automated Generation)
--------------------------------------------

**Tools:** medini analyze, SCADE Safety Architect, OSATE (AADL)

**Benefits:**
- **Automatic FMEA generation** from system models (70% time reduction)
- **Consistency:** No missing failure modes, uniform analysis
- **Traceability:** Requirements â†’ Architecture â†’ FMEA â†’ Tests
- **Update propagation:** Model change â†’ FMEA auto-updated

**Example: AADL Error Model Annex (EMV2)**

.. code-block:: aadl

   -- Brake Pressure Sensor AADL Component
   device Brake_Pressure_Sensor
   features
       pressure_out: out data port Base_Types::Float;  -- 0-200 bar
   end Brake_Pressure_Sensor;
   
   device implementation Brake_Pressure_Sensor.impl
   annex EMV2 {**
       use types ErrorLibrary;
       use behavior ErrorLibrary::Simple;
       
       error propagations
           pressure_out: out propagation {ItemOmission, ValueError};
       end propagations;
       
       component error behavior
       transitions
           Operational -[sensor_stuck]-> Failed;
           Operational -[sensor_noisy]-> Degraded;
       propagations
           Failed -[]-> pressure_out{ItemOmission};
           Degraded -[]-> pressure_out{ValueError};
       end component;
       
       properties
           EMV2::OccurrenceDistribution => [ProbabilityValue => 1.0e-5; 
                                            Distribution => Poisson;] 
                                            applies to sensor_stuck;
   **};
   end Brake_Pressure_Sensor.impl;

**Automated FMEA Output (from OSATE tool):**

.. code-block:: text

   Component: Brake_Pressure_Sensor
   Failure Mode: sensor_stuck (transition to Failed state)
   Effect: pressure_out{ItemOmission} â†’ No pressure signal
   Probability: 1.0e-5 /hour (10 FIT)
   Propagation: Affects Brake_ECU.pressure_input
   End Effect: No braking force â†’ Collision

**70% time reduction** compared to manual FMEA creation.

6.2 Dynamic FMEA (Runtime Monitoring)
--------------------------------------

**Concept:** Traditional FMEA is **static** (analysis at design time). **Dynamic FMEA** updates during operation based on:
- **Field data:** Actual failure rates from fleet
- **Usage profile:** Real-world exposure, environmental conditions
- **Prognostics:** Remaining useful life (RUL) predictions

**Example: Connected Vehicle Fleet FMEA**

.. code-block:: python

   # Dynamic FMEA with fleet data update
   import numpy as np
   
   class DynamicFMEA:
       def __init__(self, component, failure_mode, initial_lambda):
           self.component = component
           self.failure_mode = failure_mode
           self.lambda_fit = initial_lambda  # Initial design estimate (FIT)
           self.field_failures = []
           self.operating_hours = 0
       
       def update_from_fleet(self, failures, hours):
           """Update failure rate from fleet data"""
           self.field_failures.extend(failures)
           self.operating_hours += hours
           
           # Maximum likelihood estimate
           total_failures = len(self.field_failures)
           if self.operating_hours > 0:
               self.lambda_fit = (total_failures / self.operating_hours) * 1e9  # FIT
       
       def predict_mtbf(self):
           """Mean Time Between Failures"""
           if self.lambda_fit > 0:
               return 1e9 / self.lambda_fit  # hours
           return float('inf')
       
       def risk_score(self, severity, detection_coverage):
           """Dynamic RPN based on actual failure rate"""
           occurrence = self.occurrence_rating(self.lambda_fit)
           detection = 10 - int(detection_coverage * 10)  # Invert DC to detection rating
           return severity * occurrence * detection
       
       def occurrence_rating(self, lambda_fit):
           """Map failure rate to occurrence rating (1-10)"""
           if lambda_fit < 0.1: return 1
           elif lambda_fit < 1: return 2
           elif lambda_fit < 5: return 3
           elif lambda_fit < 10: return 4
           elif lambda_fit < 20: return 5
           elif lambda_fit < 50: return 6
           elif lambda_fit < 100: return 7
           elif lambda_fit < 200: return 8
           elif lambda_fit < 500: return 9
           else: return 10
   
   # Example usage
   brake_sensor = DynamicFMEA(
       component="Brake_Pressure_Sensor",
       failure_mode="Stuck_at_zero",
       initial_lambda=10  # 10 FIT design estimate
   )
   
   # Simulate 1 million vehicle-hours of operation
   # 3 failures observed in field
   brake_sensor.update_from_fleet(failures=[1, 1, 1], hours=1_000_000)
   
   print(f"Initial Î»: {10} FIT (design)")
   print(f"Field Î»: {brake_sensor.lambda_fit:.2f} FIT (actual)")
   print(f"MTBF: {brake_sensor.predict_mtbf():.0f} hours")
   print(f"Dynamic RPN: {brake_sensor.risk_score(severity=10, detection_coverage=0.95)}")
   
   # Output:
   # Initial Î»: 10 FIT (design)
   # Field Î»: 3.00 FIT (actual) â†’ Better than design!
   # MTBF: 333,333,333 hours
   # Dynamic RPN: 30 (Severity 10 Ã— Occurrence 3 Ã— Detection 1)

================================================================================
7. FMEA Best Practices and Common Pitfalls (2026)
================================================================================

7.1 Best Practices
------------------

**1. Cross-Functional Team:**
   - Design engineers, safety engineers, quality, field service, suppliers
   - Diverse perspectives find more failure modes

**2. Iterative Process:**
   - Concept FMEA (high-level)
   - Design FMEA (detailed)
   - Integration FMEA (system interactions)
   - Update with field data, design changes

**3. Traceability:**
   - Link FMEA to requirements, hazard analysis (HARA), safety cases
   - Tool support: Jama, Polarion, DOORS, Jira

**4. Quantitative When Possible:**
   - Use failure rate databases: IEC TR 62380, MIL-HDBK-217, SN 29500
   - Field data preferred over handbook estimates

**5. Focus on High-Risk Items:**
   - Pareto principle: 20% of failure modes cause 80% of risk
   - Prioritize High AP / RPN > 125

**6. Validation:**
   - Fault injection testing (verify detection mechanisms work)
   - FMEA review meetings (peer review)
   - Independent assessment (TÃœV, Exida, UL)

7.2 Common Pitfalls
-------------------

**Pitfall 1: Incomplete Failure Mode List**
- **Problem:** Missing failure modes (especially software, security, environmental)
- **Solution:** Use checklists, historical data, brainstorming, STPA/HAZOP

**Pitfall 2: Over-Optimistic Detection Ratings**
- **Problem:** Claiming high detection when diagnostics are weak
- **Solution:** Fault injection validation, diagnostic coverage analysis

**Pitfall 3: RPN Threshold Rigidity**
- **Problem:** Treating RPN=125 as absolute cutoff (false precision)
- **Solution:** Use AIAG-VDA Action Priority (qualitative High/Medium/Low)

**Pitfall 4: No Follow-Up on Actions**
- **Problem:** FMEA becomes "paper exercise" with no design improvements
- **Solution:** Track actions in project management tool, verify effectiveness

**Pitfall 5: Static FMEA (Never Updated)**
- **Problem:** FMEA outdated after design changes, field data
- **Solution:** Living document, configuration management, dynamic FMEA

**Pitfall 6: Ignoring Common Cause Failures**
- **Problem:** Redundant channels fail together (fire, flood, software bug)
- **Solution:** Add CCF analysis (beta-factor method), diverse redundancy

================================================================================
8. Exam Preparation â€” 5 Comprehensive Questions
================================================================================

**Question 1: FMEA vs FMEDA â€” Explain Differences**

**Answer:**
- **FMEA (Failure Mode and Effects Analysis):**
  - Qualitative/semi-quantitative safety analysis
  - Identifies failure modes, causes, effects
  - Uses Severity-Occurrence-Detection (SOD) â†’ RPN
  - Applicable to design, process, software
  
- **FMEDA (FMEA + Diagnostic Analysis):**
  - Quantitative analysis for **random hardware failures**
  - Required by **ISO 26262** (automotive functional safety)
  - Calculates **SPFM** (Single-Point Fault Metric), **LFM** (Latent Fault Metric)
  - Categorizes faults: Safe, Single-point, Residual, Latent, Detected
  - Demonstrates ASIL hardware metrics compliance (e.g., SPFM â‰¥99% for ASIL D)

**Key Difference:** FMEDA quantifies **diagnostic coverage** and **safety metrics**, while traditional FMEA focuses on qualitative risk prioritization.

---

**Question 2: Calculate SPFM for ASIL D Microcontroller**

**Given:**
- Microcontroller total failure rate: Î»_total = 100 FIT
- Safe faults: Î»_safe = 10 FIT
- Single-point faults (undetected): Î»_SPF = 15 FIT
- Residual faults (detected): Î»_RF_detected = 60 FIT (DC = 95%)
- Residual faults (undetected): Î»_RF_undetected = 3 FIT
- Latent faults: Î»_MPF = 12 FIT

**Question:** Does this MCU meet ASIL D SPFM target (â‰¥99%)?

**Answer:**

.. code-block:: python

   # SPFM calculation
   lambda_SPF_total = 15  # Single-point (undetected)
   lambda_RF_total = 60 + 3  # Residual (detected + undetected)
   lambda_RF_detected = 60
   
   # Total dangerous faults
   dangerous_faults = lambda_SPF_total + lambda_RF_total  # 15 + 63 = 78 FIT
   
   # Detected dangerous faults (SPF are undetected by definition)
   detected_faults = lambda_RF_detected  # 60 FIT
   
   # SPFM formula
   SPFM = (detected_faults / dangerous_faults) * 100
   print(f"SPFM = {SPFM:.2f}%")
   
   # Output: SPFM = 76.92%
   
   # Conclusion: FAIL (need â‰¥99% for ASIL D)

**Improvement:** Reduce Î»_SPF by adding diagnostics (e.g., lockstep cores detect Î»_SPF â†’ convert to Î»_RF_detected).

---

**Question 3: AIAG-VDA 2019 Action Priority â€” Determine AP**

**Given:**
- Failure Mode: Fuel pump relay stuck open
- Severity: 8 (engine stall on highway)
- Occurrence: 6 (moderate - relay wear-out)
- Detection: 7 (low - no direct monitoring)

**Question:** What is the Action Priority (High/Medium/Low)?

**Answer:**

Using AIAG-VDA 2019 AP table:
- Severity = 8 (7-8 range)
- Occurrence = 6 (4-10 range)
- Detection = 7 (4-10 range)

**Rule:** Severity 7-8 + Occurrence 4-10 + Detection 4-10 â†’ **Action Priority = HIGH**

**Actions Required:**
- Add fuel pump current monitoring (improve detection â†’ 1-3)
- Redundant relay or fail-safe valve
- Reduce occurrence (higher quality relay, de-rating)

---

**Question 4: Software FMEA â€” API Timeout Mitigation**

**Scenario:** Autonomous vehicle lidar processing API times out (>100ms deadline miss).

**Question:** Perform Software FMEA and propose mitigation.

**Answer:**

.. code-block:: text

   Failure Mode: Lidar API timeout (deadline miss >100ms)
   
   Causes:
   â”œâ”€ High point cloud density (urban environment)
   â”œâ”€ CPU overload (resource starvation)
   â””â”€ Memory fragmentation (allocation delays)
   
   Effects:
   â”œâ”€ Local: Missed detection cycle
   â”œâ”€ System: Stale object list used for path planning
   â”œâ”€ End: Delayed emergency braking â†’ Collision (Severity = 10)
   
   Current Controls:
   â”œâ”€ Prevention: Deadline monotonic scheduling
   â”œâ”€ Detection: Watchdog timer (detects timeout)
   
   Occurrence: 5 (moderate)
   Detection: 4 (watchdog detects, but recovery slow)
   RPN: 10 Ã— 5 Ã— 4 = 200 â†’ CRITICAL
   
   Mitigations:
   1. **Timeout fallback:** Use radar-only object list if lidar times out
   2. **Adaptive processing:** Reduce point cloud resolution if CPU>80%
   3. **Graceful degradation:** L3 â†’ L2 (request driver takeover)
   4. **Resource reservation:** Guarantee lidar 50% CPU minimum
   
   After Mitigation:
   â”œâ”€ Occurrence: 2 (rare with adaptive processing)
   â”œâ”€ Detection: 2 (fast radar fallback)
   â””â”€ RPN': 10 Ã— 2 Ã— 2 = 40 â†’ ACCEPTABLE

---

**Question 5: FMEA-MSR for OOD Detection in AI/ML**

**Scenario:** Autonomous vehicle camera DNN detects unknown object (kangaroo on highway).

**Question:** Apply FMEA-MSR (Monitoring + System Response) framework.

**Answer:**

.. code-block:: text

   System: L3 Highway Pilot
   Component: Camera Object Detection DNN
   
   Failure Mode: Out-of-Distribution (OOD) input
   Triggering Condition: Kangaroo on highway (not in training dataset)
   
   Effects:
   â”œâ”€ DNN misclassifies (low confidence or wrong class)
   â”œâ”€ Path planning ignores object
   â””â”€ Collision with kangaroo â†’ Vehicle damage, injury (Severity = 7)
   
   Monitoring Mechanisms:
   â”œâ”€ DNN confidence score < 0.5 (low confidence threshold)
   â”œâ”€ OOD detector (Mahalanobis distance, ODIN score)
   â”œâ”€ Radar cross-check (object present but no DNN detection?)
   â””â”€ Unexpected object size/velocity
   
   System Response (Graduated):
   1. **Confidence 0.3-0.5:** Increase radar/lidar fusion weight
   2. **Confidence < 0.3:** Treat as "unknown obstacle" â†’ Emergency braking
   3. **OOD detector triggers:** Reduce speed to 60 km/h, alert driver
   4. **Persistent OOD:** Request takeover, L3 â†’ L2 mode
   
   Response Effectiveness:
   â”œâ”€ Detection latency: 150ms (OOD detector)
   â”œâ”€ Braking response: 200ms actuation
   â”œâ”€ Safe state: Emergency stop or successful avoidance
   â””â”€ False positive rate: < 2% (acceptable for safety)
   
   Action Priority: HIGH (Severity 7, safety-critical)
   
   Actions:
   â”œâ”€ Improve training data (add rare animals, edge cases)
   â”œâ”€ OOD detector calibration (reduce false negatives)
   â”œâ”€ Multi-modal fusion (thermal camera for night animals)
   â””â”€ Driver monitoring (ensure takeover readiness)

================================================================================
9. Completion Checklist
================================================================================

.. code-block:: text

   âœ… FMEA Fundamentals
      â”œâ”€ Definition, purpose, standards (ISO 26262, IEC 61508, AIAG-VDA)
      â”œâ”€ FMEA vs FMECA vs FMEDA (differences and applications)
      â””â”€ 7-step FMEA process (scope, functions, failure modes, effects, controls, risk)
   
   âœ… DFMEA (Design FMEA)
      â”œâ”€ AIAG-VDA 2019 harmonized approach (7 steps)
      â”œâ”€ Action Priority (High/Medium/Low) vs RPN
      â””â”€ Automotive brake pressure sensor example
   
   âœ… FMEDA (ISO 26262)
      â”œâ”€ SPFM and LFM metrics (formulas, targets)
      â”œâ”€ Fault classification (Safe, SPF, RF, MPF)
      â””â”€ ASIL D microcontroller calculation example
   
   âœ… FMEA-MSR (2026 Evolution)
      â”œâ”€ Monitoring and System Response framework
      â”œâ”€ Autonomous driving camera failure example
      â””â”€ AI/ML OOD detection FMEA-MSR
   
   âœ… Software FMEA
      â”œâ”€ API timeout, buffer overflow examples
      â”œâ”€ Systematic vs random failures
      â””â”€ Corrected code with bounds checking
   
   âœ… Advanced Techniques
      â”œâ”€ Model-Based FMEA (AADL EMV2, 70% time reduction)
      â”œâ”€ Dynamic FMEA (fleet data updates)
      â””â”€ Best practices and common pitfalls
   
   âœ… Exam Questions (5)
      â”œâ”€ FMEA vs FMEDA differences
      â”œâ”€ SPFM calculation for ASIL D
      â”œâ”€ AIAG-VDA Action Priority determination
      â”œâ”€ Software FMEA (API timeout)
      â””â”€ FMEA-MSR for AI/ML OOD

================================================================================
10. Key Takeaways
================================================================================

1. **FMEA is bottom-up safety analysis** identifying failure modes at component level, assessing effects, and prioritizing corrective actions (RPN or AP).

2. **FMEDA (ISO 26262) quantifies random hardware safety** via SPFM (â‰¥99% ASIL D) and LFM (â‰¥90% ASIL D) metrics, categorizing faults as Safe/SPF/RF/MPF.

3. **AIAG-VDA 2019 replaces RPN with Action Priority** (High/Medium/Low) for more qualitative, less misleading risk assessment.

4. **FMEA-MSR (2026) adds Monitoring + System Response** for runtime failures in autonomous systems, addressing OOD, adversarial attacks, and dynamic hazards.

5. **Software FMEA focuses on systematic failures** (API timeouts, buffer overflows, race conditions) not random hardware faults, requiring different mitigation strategies.

6. **Model-Based FMEA (AADL EMV2) achieves 70% time reduction** through automated generation, consistency, and traceability from architecture models.

7. **FMEA is a living document** requiring updates from field data, design changes, and dynamic fleet monitoring for continuous safety improvement.

================================================================================

**Document Version:** 1.0  
**Last Updated:** January 16, 2026  
**Author:** GitHub Copilot (Claude Sonnet 4.5)  
**Standards:** ISO 26262:2018, IEC 61508:2010, AIAG-VDA FMEA 2019, MIL-STD-1629A

================================================================================
