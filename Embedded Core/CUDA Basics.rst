ğŸš€ **CUDA Basics Cheatsheet** ğŸš€ (GPU Parallel Programming)
==============================================================

A **concise guide** to CUDA parallel computing on NVIDIA GPUs (CUDA Toolkit ~13.x, 2025â€“2026). Master the essential programming model, synchronization, memory hierarchy, and performance patterns.

**Official Reference**: CUDA C++ Programming Guide (v13.1+) at https://docs.nvidia.com/cuda/cuda-c-programming-guide/

---

ğŸ—ï¸ **1. Core Concepts & Execution Hierarchy**
==============================================

ğŸ—ï¸ **1. Core Concepts & Execution Hierarchy**
==============================================

The **CUDA execution model** is hierarchical. Understand these 4 layers:

| ğŸ¯ Concept        | ğŸ“– Description                                        | ğŸ“Š Max Limits (GPU-dependent) | ğŸ–¥ï¸ Built-in Variables      |
|-------------------|-------------------------------------------------------|-----------------------------|-----------------------|
| **Thread** ğŸ”µ      | Single task (smallest unit)                          | â€”                            | `threadIdx.x/y/z`     |
| **Warp** ğŸ“¦        | 32 threads executing in lock-step (SIMT)             | â€”                           | `warpSize` (always 32)|
| **Block** ğŸ§±       | Group of warps (shared memory, sync possible)         | 1024 threads/block          | `blockIdx`, `blockDim` |
| **Grid** ğŸŒ        | Collection of blocks (independent, parallel)          | Millions of blocks          | `gridDim`             |

**Memory Hierarchy Visualization**:

.. code-block:: text

   ğŸŒ Grid
   â”œâ”€ ğŸ§± Block 0 (shared memory, 48 KB)
   â”‚  â”œâ”€ ğŸ“¦ Warp 0 (32 threads)
   â”‚  â”‚  â”œâ”€ ğŸ”µ Thread 0 (registers: ~255 bytes)
   â”‚  â”‚  â”œâ”€ ğŸ”µ Thread 1
   â”‚  â”‚  â””â”€ ...
   â”‚  â””â”€ ğŸ“¦ Warp 1
   â”‚
   â”œâ”€ ğŸ§± Block 1
   â”‚  â””â”€ ...
   â”‚
   â””â”€ ... (millions of blocks)
   
   ğŸ§  Global Memory (device DRAM, multiple GB)
   âš¡ Constant Memory (64 KB, read-only cache)

**Key Points**:
   âœ… Threads in a **block** can synchronize via `__syncthreads()` and access shared memory
   âœ… Threads in different **blocks** are independent (no direct sync!)
   âœ… **Warps** execute in lock-step â†’ minimize branch divergence
   âœ… 32 = magic number (warp size = 32 on all modern NVIDIA GPUs)

ğŸ’» **2. Kernel Launch Syntax** (The Magic Formula)
==================================================

```cuda
kernelName<<<gridDim, blockDim, sharedMemBytes, stream>>>(args...);
```

**Parameters** ğŸ¯:

   **gridDim** (number of blocks):
   - Type: `dim3` or `int` 
   - Example: `<<<256, ...>>>` â†’ 256 blocks
   - Example: `<<<dim3(32, 32, 1), ...>>>` â†’ 32Ã—32 blocks (2D grid)
   - Can be 1D, 2D, or 3D

   **blockDim** (threads per block):
   - Type: `dim3` or `int`
   - Example: `<<<..., 256>>>` â†’ 256 threads/block
   - Example: `<<<..., dim3(16, 16, 1)>>>` â†’ 16Ã—16 threads (2D block)
   - âš ï¸ **Max 1024 threads/block** (product of dimensions)

   **sharedMemBytes** (optional, dynamic shared memory):
   - Size in bytes of shared memory to allocate
   - Default: 0
   - Example: `<<<..., ..., 4096>>>` â†’ 4 KB shared mem/block

   **stream** (optional, async execution):
   - CUDA stream handle (enables async operations)
   - Default: stream 0 (legacy default stream)
   - Example: `<<<..., ..., 0, myStream>>>` â†’ uses custom stream

**Practical Examples** ğŸ”§:

.. code-block:: cuda

   // 1ï¸âƒ£ Vector addition (1D, 1 million elements)
   int N = 1000000;
   int threadsPerBlock = 256;
   int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;
   vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);
   // Result: ~3906 blocks Ã— 256 threads = 1M threads total
   
   // 2ï¸âƒ£ Matrix multiply (2D grid, 2D block)
   dim3 blockDim(16, 16);  // 256 threads/block
   dim3 gridDim((width + 15)/16, (height + 15)/16);
   matmul2D<<<gridDim, blockDim>>>(d_A, d_B, d_C, width, height);
   
   // 3ï¸âƒ£ With shared memory & stream
   size_t smemBytes = 2048;
   cudaStream_t stream;
   cudaStreamCreate(&stream);
   myKernel<<<256, 128, smemBytes, stream>>>(d_data);
   cudaStreamSynchronize(stream);  // Wait for completion

ğŸ“ **3. Thread Indexing Formulas** (Critical for Correctness!)
==============================================================

| ğŸ¯ Purpose                        | ğŸ“ Calculation                    | ğŸ’¡ When to Use                   |
|-----------------------------------|-----------------------------------|---------------------------------|
| **1D Global Thread ID** ğŸ”µ        | `int idx = blockIdx.x * blockDim.x + threadIdx.x;` | Vectors, 1D arrays |
| **1D with Bounds Check** âœ…       | `if (idx < N) { ... }`            | **ALWAYS DO THIS!** Prevents out-of-bounds |
| **2D Global (Row-Major)** ğŸ–¼ï¸     | `int x = blockIdx.x * blockDim.x + threadIdx.x;`<br>`int y = blockIdx.y * blockDim.y + threadIdx.y;`<br>`int idx = y * width + x;` | Images, matrices |
| **Strided Loop** ğŸ”„              | `for (int i = idx; i < N; i += gridDim.x * blockDim.x)` | Processes all data, good occupancy |

**Key Insight**: Total threads = `gridDim.x * blockDim.x`, so one thread per element:

.. code-block:: cuda

   // âœ… CORRECT: Simple vector kernel
   __global__ void vectorAdd(float *A, float *B, float *C, int N) {
       int idx = blockIdx.x * blockDim.x + threadIdx.x;
       if (idx < N) {
           C[idx] = A[idx] + B[idx];  // One thread per element
       }
   }

   // ğŸ”„ ADVANCED: Strided loop (process more elements if N >> total threads)
   __global__ void vectorAddStrided(float *A, float *B, float *C, int N) {
       int idx = blockIdx.x * blockDim.x + threadIdx.x;
       int stride = gridDim.x * blockDim.x;
       for (int i = idx; i < N; i += stride) {
           C[i] = A[i] + B[i];
       }
   }

   // 2ï¸âƒ£ 2D Kernel (images)
   __global__ void blur2D(float *input, float *output, int width, int height) {
       int x = blockIdx.x * blockDim.x + threadIdx.x;
       int y = blockIdx.y * blockDim.y + threadIdx.y;
       if (x < width && y < height) {
           int idx = y * width + x;  // Row-major addressing
           output[idx] = ... input calculations ...
       }
   }

ğŸ’¾ **4. Memory Hierarchy & Performance**
========================================

The **memory pyramid** (top = fast but small, bottom = slow but large):

.. code-block:: text

   Speed:   âš¡âš¡âš¡âš¡âš¡     Size:    ~256 bytes/thread
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚    Registers     â”‚  Automatic, per-thread
   â”‚   (L1 cache)     â”‚  Fastest! (single cycle)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â¬‡ï¸
   Speed:   âš¡âš¡âš¡âš¡      Size:    48â€“96 KB/block
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Shared Memory   â”‚  `__shared__` declared
   â”‚   (L2 cache)     â”‚  Fast (4â€“8 cycles)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â¬‡ï¸
   Speed:   âš¡âš¡âš¡       Size:    64 KB (read-only)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Constant Mem    â”‚  `__constant__` declared
   â”‚  (cached)        â”‚  Broadcast to warp
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â¬‡ï¸
   Speed:   âš¡âš¡        Size:    8â€“40 GB
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Global Memory  â”‚  Allocated with cudaMalloc
   â”‚   (device DRAM)  â”‚  Slow (100â€“200 cycles) âš ï¸
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

**Memory Types** ğŸ“Š:

| ğŸ§  Type           | ğŸ”’ Scope / Lifetime       | âš¡ Speed    | ğŸ“ Size | ğŸ–Šï¸ Qualifier / How           | ğŸ”— Coherent? |
|-------------------|--------------------------|-----------|--------|----------------------------|----|
| **Registers**     | Per thread               | âš¡âš¡âš¡âš¡âš¡| ~256 B | Automatic (local variables) | âœ…  |
| **Shared**        | Per block                | âš¡âš¡âš¡âš¡  | 48â€“96 KB | `__shared__ float s[256];` | âœ… (block-level) |
| **Constant**      | Global read-only         | âš¡âš¡âš¡   | 64 KB | `__constant__ float c[1024];` | âœ… |
| **Global**        | Global (device DRAM)     | âš¡âš¡    | ~GB | `cudaMalloc` or kernel param | âœ… (with sync) |
| **Local**         | Per thread (spill)       | âš¡âš¡    | Device DRAM | Automatic (overflow from regs) | âœ… |
| **Texture/Read-only** | Read-cached     | âš¡âš¡âš¡   | Device DRAM | `texture<>` or `__restrict__` | âœ… |

**Unified / Managed Memory** (convenience, slower):

.. code-block:: cuda

   // Accessible from both host & device automatically
   float *ptr;
   cudaMallocManaged(&ptr, size);  // âš ï¸ Slower due to page migration
   ptr[0] = 1.0;  // Host access
   kernel<<<...>>>(ptr);  // Device access
   cudaFree(ptr);

**Best Practices**:
   âœ… Keep hot data in **shared memory** (reuse within block)
   âœ… Coalesce **global memory** accesses (threads â†’ sequential addresses)
   âœ… Use **constant memory** for read-only config data
   âœ… Minimize **divergence** to keep threads in sync

ğŸ”§ **5. Memory Management API**
==============================

**Allocation & Deallocation** ğŸ› ï¸:

.. code-block:: cuda

   // Allocate device memory
   float *d_ptr;
   cudaMalloc(&d_ptr, size);  // size in bytes
   if (d_ptr == NULL) { /* error! */ }
   
   // Free device memory (MUST do!)
   cudaFree(d_ptr);
   d_ptr = NULL;  // Good practice

**Host â†” Device Transfer** ğŸ”„:

.. code-block:: cuda

   // Synchronous (blocks host until done)
   cudaMemcpy(d_ptr, h_ptr, size, cudaMemcpyHostToDevice);
   cudaMemcpy(h_ptr, d_ptr, size, cudaMemcpyDeviceToHost);
   
   // GPU â†” GPU transfer (P2P)
   cudaMemcpyPeer(d_ptr1, device0, d_ptr2, device1, size);
   
   // Asynchronous (returns immediately, uses stream)
   cudaMemcpyAsync(d_ptr, h_ptr, size, cudaMemcpyHostToDevice, stream);
   cudaStreamSynchronize(stream);  // Wait for completion
   
   // Zero memory (fast)
   cudaMemset(d_ptr, 0, size);

**Streams for Async Operations** âš¡:

.. code-block:: cuda

   cudaStream_t stream;
   cudaStreamCreate(&stream);
   
   // Queue operations (don't wait)
   cudaMemcpyAsync(d_A, h_A, size, cudaMemcpyHostToDevice, stream);
   kernel<<<blocks, threads, 0, stream>>>(d_A);
   cudaMemcpyAsync(h_B, d_B, size, cudaMemcpyDeviceToHost, stream);
   
   // Wait for all ops in this stream
   cudaStreamSynchronize(stream);
   
   cudaStreamDestroy(stream);

ğŸ–¥ï¸ **6. Kernel Qualifiers & Key Functions**
============================================

**Kernel & Function Qualifiers** ğŸ¯:

.. code-block:: cuda

   // Launched FROM HOST, executes ON DEVICE
   __global__ void kernel_gpu(float *data, int N) {
       int idx = blockIdx.x * blockDim.x + threadIdx.x;
       if (idx < N) data[idx] *= 2.0f;
   }
   
   // Called FROM DEVICE CODE ONLY
   __device__ float helper_gpu(float x) {
       return x * x + 1.0f;
   }
   
   // Callable FROM BOTH HOST & DEVICE
   __host__ __device__ float dual(float x) {
       return x + 1.0f;
   }

**Synchronization & Memory Barriers** ğŸ”’:

.. code-block:: cuda

   // Block-level barrier (all threads in block wait here)
   __syncthreads();
   
   // Memory ordering (thread fence)
   __threadfence();        // Global memory fence
   __threadfence_block();  // Block-level fence
   
   // Warp-level shuffle (communicate within 32-thread warp)
   int result = __shfl_down_sync(0xFFFFFFFF, value, 1);  // Shift right by 1

**Block Primitives** (cooperative operations):

.. code-block:: cuda

   // Warp-level operations
   int warp_sum = __reduce_add_sync(mask, value);  // Reduce across warp
   
   // Block-level operations (requires cooperative groups)
   #include <cooperative_groups.h>
   __global__ void kernel_coop() {
       cg::thread_block block = cg::this_thread_block();
       block.sync();  // Synchronize all threads
   }

ğŸ›¡ï¸ **7. Error Handling (Essential!)**
======================================

**CUDA Error Checking Macro** ğŸ”´:

.. code-block:: cuda

   #define CUDA_CHECK(call) do { \
       cudaError_t err = (call); \
       if (err != cudaSuccess) { \
           fprintf(stderr, "CUDA Error at %s:%d\n", __FILE__, __LINE__); \
           fprintf(stderr, "  Error: %s\n", cudaGetErrorString(err)); \
           exit(1); \
       } \
   } while(0)
   
   // Usage:
   CUDA_CHECK(cudaMalloc(&d_ptr, size));
   CUDA_CHECK(cudaMemcpy(d_ptr, h_ptr, size, cudaMemcpyHostToDevice));
   CUDA_CHECK(cudaDeviceSynchronize());  // Check kernel errors too!

**Kernel Launch Error Pattern** âœ…:

.. code-block:: cuda

   // Kernels return void, check async error separately
   kernel<<<blocks, threads>>>(args);
   CUDA_CHECK(cudaGetLastError());        // Check launch error
   CUDA_CHECK(cudaDeviceSynchronize());   // Check execution error

**Common Errors & Fixes**:

   âŒ **cudaErrorInvalidConfiguration**: Block size > 1024 threads
      âœ… Fix: Reduce `blockDim.x * blockDim.y * blockDim.z â‰¤ 1024`

   âŒ **cudaErrorMemoryAllocation**: Out of device memory
      âœ… Fix: Reduce allocation size, free unused memory

   âŒ **cudaErrorInvalidResourceHandle**: Accessing invalid memory
      âœ… Fix: Ensure `idx < N` bounds check, no double-free

   âŒ **cudaErrorInvalidMemcpyDirection**: Wrong direction
      âœ… Fix: Use correct `cudaMemcpyHostToDevice` or `cudaMemcpyDeviceToHost`

âš¡ **8. Performance Optimization Quick Reference**
==================================================

**Occupancy & Thread Blocks** ğŸ¯:

   ğŸ”¹ **What is occupancy?** Ratio of active threads to max possible threads
   ğŸ”¹ **Why? More occupancy = better latency hiding**
   ğŸ”¹ **Good target: 50â€“75% occupancy**

   âœ… **Tips**:
      - Block size: Multiple of 32 (warp size)
      - Avoid prime numbers like 65, 128 (use 64 or 128 instead)
      - Use `nvidia-smi` or Nsight Compute to measure actual occupancy

**Memory Coalescing** ğŸ“ (Most Critical!):

   âœ… **Best Case** (coalesced):
      Thread 0 reads from address 0x0000
      Thread 1 reads from address 0x0004 (sequential!)
      Thread 2 reads from address 0x0008
      â†’ Single memory transaction âš¡

   âŒ **Bad Case** (uncoalesced):
      Thread 0 reads from 0x0000
      Thread 1 reads from 0x1000 (random!)
      Thread 2 reads from 0x0010
      â†’ Multiple slow transactions ğŸ¢

   **Fix**: Ensure consecutive threads access consecutive memory addresses

**Shared Memory Optimization** ğŸ§ :

.. code-block:: cuda

   __global__ void matmul_optimized(float *A, float *B, float *C, int N) {
       __shared__ float sA[16][16];  // Shared tile
       __shared__ float sB[16][16];
       
       // Load A tile into shared memory
       sA[threadIdx.y][threadIdx.x] = A[...];
       __syncthreads();  // Wait for all threads
       
       // Compute using fast shared memory
       for (int i = 0; i < 16; i++) {
           sum += sA[threadIdx.y][i] * sB[i][threadIdx.x];
       }
       
       C[...] = sum;
   }

**Common Bottlenecks & Fixes**:

   | ğŸš« Problem | ğŸ”§ Solution |
   |-----------|-----------|
   | Low occupancy | Increase block size (if registers allow) |
   | Uncoalesced memory | Reorganize data layout for sequential access |
   | Bank conflicts (shared mem) | Use padding: `__shared__ float s[32][33]` not `[32][32]` |
   | Branch divergence | Minimize `if` statements inside warps |
   | Slow algorithms | Use NVIDIA libraries (cuBLAS, cuDNN, etc.) |

**Profiling Tools** ğŸ”:

   - **Nsight Compute**: Deep kernel profiling (register usage, memory, etc.)
   - **Nsight Systems**: Whole application timeline
   - **nvprof**: Legacy profiler (still works)
   - **nvidia-smi dmon**: Real-time GPU monitoring

   Command:
   ```bash
   ncu --set full ./my_cuda_app  # Profile with Nsight Compute
   nsys profile ./my_cuda_app     # Profile with Nsight Systems
   ```

This cheatsheet targets everyday CUDA C/C++ usage in 2026 â€” let me know if you want expansions on cooperative groups, streams, graph execution, or CUDA 13.x+ specifics!
ğŸ“‹ **9. Complete Hello World Example**
======================================

.. code-block:: cuda

   #include <stdio.h>
   #include <cuda_runtime.h>
   
   #define CUDA_CHECK(call) do { \
       cudaError_t err = (call); \
       if (err != cudaSuccess) { \
           printf("CUDA error: %s\n", cudaGetErrorString(err)); \
           exit(1); \
       } \
   } while(0)
   
   // âœ… Simple kernel: add two vectors
   __global__ void vectorAdd(float *A, float *B, float *C, int N) {
       int idx = blockIdx.x * blockDim.x + threadIdx.x;
       if (idx < N) {
           C[idx] = A[idx] + B[idx];
       }
   }
   
   int main() {
       int N = 1000000;
       size_t bytes = N * sizeof(float);
       
       // Host memory
       float *h_A = (float *)malloc(bytes);
       float *h_B = (float *)malloc(bytes);
       float *h_C = (float *)malloc(bytes);
       
       // Initialize host arrays
       for (int i = 0; i < N; i++) {
           h_A[i] = 1.0f;
           h_B[i] = 2.0f;
       }
       
       // Device memory
       float *d_A, *d_B, *d_C;
       CUDA_CHECK(cudaMalloc(&d_A, bytes));
       CUDA_CHECK(cudaMalloc(&d_B, bytes));
       CUDA_CHECK(cudaMalloc(&d_C, bytes));
       
       // Copy host â†’ device
       CUDA_CHECK(cudaMemcpy(d_A, h_A, bytes, cudaMemcpyHostToDevice));
       CUDA_CHECK(cudaMemcpy(d_B, h_B, bytes, cudaMemcpyHostToDevice));
       
       // Launch kernel: 256 threads/block, ~3906 blocks
       int threadsPerBlock = 256;
       int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;
       vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);
       CUDA_CHECK(cudaGetLastError());
       CUDA_CHECK(cudaDeviceSynchronize());
       
       // Copy device â†’ host
       CUDA_CHECK(cudaMemcpy(h_C, d_C, bytes, cudaMemcpyDeviceToHost));
       
       // Verify result
       for (int i = 0; i < 5; i++) {
           printf("C[%d] = %.2f (expected 3.0)\n", i, h_C[i]);
       }
       
       // Cleanup
       cudaFree(d_A);
       cudaFree(d_B);
       cudaFree(d_C);
       free(h_A);
       free(h_B);
       free(h_C);
       
       printf("âœ… Success!\n");
       return 0;
   }

**Compilation**:

.. code-block:: bash

   # Compile with nvcc (NVIDIA CUDA Compiler)
   nvcc -o vector_add vector_add.cu
   ./vector_add

---

ğŸ¯ **10. Decision Tree: Choosing Block Size**
=============================================

.. code-block:: text

   Start: How much shared memory do you need?
   â”‚
   â”œâ”€ NO shared memory?
   â”‚  â””â”€ Pick 256 (very common, good balance)
   â”‚     â””â”€ Performance acceptable?
   â”‚        â”œâ”€ YES â†’ Done! ğŸ‰
   â”‚        â””â”€ NO â†’ Try 512 or 128
   â”‚
   â”œâ”€ 4 KB shared memory?
   â”‚  â””â”€ Pick 256â€“512 (allows 2â€“4 blocks/SM)
   â”‚
   â”œâ”€ 16 KB shared memory?
   â”‚  â””â”€ Pick 128â€“256 (allows 1â€“2 blocks/SM)
   â”‚
   â””â”€ 32+ KB shared memory?
      â””â”€ Pick 128 or less (serializes blocks)

âœ… **Golden Rules**:
   1. Thread block size must be multiple of 32 âœ…
   2. Max 1024 threads/block
   3. Aim for 50â€“75% occupancy
   4. Use Nsight Compute to measure actual occupancy
   5. When in doubt, start with 256 ğŸ¯

---

ğŸš€ **11. Key Takeaways (TL;DR)**
==================================

âœ… **Hierarchy**: Thread (idx) â†’ Block (shared mem) â†’ Grid (independent)
âœ… **Index Formula**: `idx = blockIdx.x * blockDim.x + threadIdx.x`
âœ… **Memory**: Registers (fast) â†’ Shared (fast) â†’ Global (slow)
âœ… **Coalescing**: Consecutive threads â†’ consecutive addresses = speed! âš¡
âœ… **Sync**: `__syncthreads()` only works within a block
âœ… **Error Handling**: ALWAYS use CUDA_CHECK macro
âœ… **Occupancy**: 50â€“75% is good, profile with Nsight Compute
âœ… **Profiling**: Don't guess, measure with tools!
âœ… **Block Size**: Start with 256, multiple of 32
âœ… **Shared Memory**: Use for data reuse, avoid bank conflicts

---

ğŸ“š **12. Essential Resources**
==============================

ğŸ”— **Official Documentation**:
   - CUDA C++ Programming Guide: https://docs.nvidia.com/cuda/cuda-c-programming-guide/
   - CUDA Runtime API: https://docs.nvidia.com/cuda/cuda-runtime-api/
   - Optimizing CUDA Kernels: https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/

ğŸ“– **Learning Resources**:
   - "Even Easier Introduction to CUDA" (NVIDIA blog)
   - Udacity FREE course: "Intro to Parallel Programming" (with CUDA)
   - Mark Harris' presentations on CUDA performance

ğŸ› ï¸ **Tools**:
   - **Nsight Compute** (2025.5+): Deep kernel profiling
   - **Nsight Systems**: Full application timeline
   - **NVIDIA Samples**: Code examples in CUDA Toolkit installation

ğŸ–¥ï¸ **Development**:
   - **NVCC**: NVIDIA C/C++ Compiler (`nvcc file.cu -o output`)
   - **CMake + CUDA**: For complex projects
   - **Conda/pip**: `cupy` or `pycuda` for Python

---

âœ¨ **Final Note**: This cheatsheet covers CUDA 13.x (2025â€“2026). For latest features (CUDA 12.4+), check the official guide. Happy GPU programming! ğŸš€

*Last updated: 2026-01-12 | CUDA 13.1+*

